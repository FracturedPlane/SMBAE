{
    "comment__": "Type of model/network to use for the actor and critic",
"model_type": "Deep_CNN",
    "comment__": "Learning algorithm to use",
"agent_name": "CACLA",
    "comment__": "Folder to store the training data in.",
"data_folder": "Gaps_Sphere/",
	"comment": "initial probability of selecting a random action",
"epsilon": 0.80, 
	"comment": "initial probability of selecting a discrete random action",
"omega": 0.5,
    "comment__": "Batch size used for learning",
"batch_size": 32,
    "comment__": "Learning rate for the actor/policy",
"learning_rate": 0.0001,
    "comment__": "Stupid file that should not be needed anymore...",
"anchor_file": "../data/anchorData/paperGibbonAnchors.json",
    "comment__": "Config file for the simulator",
"sim_config_file": "./settings/ballgame1D/bouncegame_settings_sphere_gaps.json",
    "comment__": "A different config file for the forward dynamics simulator",
"forwardDynamics_config_file": "./settings/ballgame1D/bouncegame_settings_sphere_gaps.json",
    "comment__": "Exploration rate use when randomly generating new actions",
"exploration_rate": 0.1,
    "comment__": "Number of rounds to perform before termination",
"rounds": 5000,
    "comment__": "Number of epochs to perform per round",
"epochs": 10,
    "comment__": "Number of epoch/episode to evaluate the policy over",
"eval_epochs": 5,
    "comment__": "Discount factor used during learning",
"discount_factor": 0.8,
    "comment__": "Should the training be plotted during learning",
"visualize_learning": true,
    "comment__": "Whether or not to save the plotted data while learning",
"save_trainData": true,
    "comment__": "Whether or not to train a forward dynamics model as well",
"train_forward_dynamics": false,
    "comment__": "Whether or not to plot the training curve while learning for the forward dynamics model",
"visulaize_forward_dynamics": false,
    "comment__": "Bounds used for scaling rewards for networks",
"reward_bounds": [[0.0],[1.0]],
    "comment__": "Max length of the Experience memory",
"expereince_length": 10000,
    "comment__": "Possible state bounds to be used for scaling states for networks",
"state_bounds": [[-0.3, -0.5264425 , -0.78969873, -0.97253482, -1.11645622,
		        -1.2002147 , -1.23023312, -1.24259293, -1.22618417, -1.25859673,
		        -1.26904543, -1.26939072, -1.22433556, -1.14766641, -1.16278922,
		        -1.12714715, -1.01828899, -0.97871991, -0.82655933, -0.81719165,
		        -0.78381916, -0.66313616, -0.60093671, -0.34565929, -0.3108273 ,
		        -0.36593349, -0.48280346, -0.58464969, -0.63556857, -0.72024995,
		        -0.78904765, -0.86414284, -0.91960968, -0.98790918, -1.06016465,
		        -1.09593826, -1.17123387, -1.21614046, -1.18665637, -1.18977873,
		        -1.16802855, -1.17641204, -1.12629747, -1.05130978, -1.01584188,
		        -0.96786458, -0.97974615, -0.91850384, -0.84496735, -0.75644984,
		        -0.71019496, -0.68593065, -0.71667464, -0.64276339, -0.66236352,
		        -0.6225864 , -0.65380763, -0.77059053, -0.7679172 , -0.85700606,
		        -0.86117694, -0.90003471, -0.97356898, -1.00498443, -1.03521092,
		        -1.01289539, -1.04424784, -1.09682272, -1.09902941, -1.14061181,
		        -1.11256998, -1.08793183, -1.07893698, -1.01535156, -1.00349236,
		        -0.96421334, -0.91850384, -0.89946836, -0.88113126, -0.8070746 ,
		        -0.79617962, -0.7584861 , -0.76523449, -0.78774384, -0.76321624,
		        -0.7745831 , -0.82655933, -0.86058244, -0.89946836, -0.86177101,
		        -0.91127862, -0.93979872, -0.96473596, -1.02751994, -1.02121563,
		        -1.02170236, -0.98485776, -0.98536714, -1.02413153, -1.03998009,
		         0.02970726, -1.11256852e+00,
         -1.18606932e+00,  -1.30560167e+00,  -8.62383101e-01,
         -9.60017487e-01,  -9.62293427e-01,  -9.10850503e-01,
          4.60514460e-01],
				 [ 0.3        ,  0.4100425 ,  0.53769873,  0.59413482,  0.61605622,
		         0.6174147 ,  0.61543312,  0.61419293,  0.61578417,  0.61219673,
		         0.61064543,  0.61059072,  0.61593556,  0.61766641,  0.61798922,
		         0.61674715,  0.60348899,  0.59551991,  0.55135933,  0.54799165,
		         0.53541916,  0.48273616,  0.45133671,  0.29325929,  0.2680273 ,
		         0.30753349,  0.38400346,  0.44264969,  0.46916857,  0.50904995,
		         0.53744765,  0.56414284,  0.58080968,  0.59750918,  0.61016465,
		         0.61433826,  0.61803387,  0.61654046,  0.61785637,  0.61777873,
		         0.61802855,  0.61801204,  0.61669747,  0.60890978,  0.60304188,
		         0.59306458,  0.59574615,  0.58050384,  0.55776735,  0.52444984,
		         0.50459496,  0.49353065,  0.50747464,  0.47276339,  0.48236352,
		         0.4625864 ,  0.47820763,  0.53019053,  0.5291172 ,  0.56180606,
		         0.56317694,  0.57523471,  0.59436898,  0.60098443,  0.60641092,
		         0.60249539,  0.60784784,  0.61442272,  0.61462941,  0.61741181,
		         0.61576998,  0.61353183,  0.61253698,  0.60295156,  0.60069236,
		         0.59221334,  0.58050384,  0.57506836,  0.56953126,  0.5442746 ,
		         0.54017962,  0.5252861 ,  0.52803449,  0.53694384,  0.52721624,
		         0.5317831 ,  0.55135933,  0.56298244,  0.57506836,  0.56337101,
		         0.57847862,  0.58619872,  0.59233596,  0.60511994,  0.60401563,
		         0.60410236,  0.59685776,  0.59696714,  0.60453153,  0.60718009,
		         0.15013274, 1.13985332e+00,
          1.00815394e+00,   1.10822419e+00,   1.04885589e+00,
          1.05575017e+00,   1.03756896e+00,   1.08150713e+00,
          3.57103108e+00]],

    "comment__": "Action scaling values to be used to scale values for the network",
"action_bounds": [[0.5],
                  [3.5]],
    "comment__": "Set of discrete actions that can be sampled from",				     
"discrete_actions": [[1.5],
                    [2.75],
                    [1.0],
                    [1.25],
                    [1.65],
                    [1.75], 
                    [2.0],
                    [3.25],
                    [2.5]],
    "comment__": "Is action space continuous or discrete?",
"action_space_continuous":true,
    "comment__": "Should the method train on the validation set only",
"train_on_validation_set":true,
    "comment__": "Name of the type of simulator to use",
"environment_type": "ballgame_1d",
    "comment__": "Model type to use for the forward dynamics model",
"forward_dynamics_predictor": "network",
    "comment__": "Method to be used for the forward dynamics model is the model types uses a simulator",
"sampling_method": "SequentialMC",
    "comment__": "Use the action suggested by the policy to start the sampling method.",
"use_actor_policy_action_suggestion": true,
    "comment__": "If selecting from a uniform distribution the number of regularly distant samples to take / action dimension",
"num_uniform_action_samples": 3,
    "comment__": "Number of steps ahead the actions should be sampled",
"look_ahead_planning_steps": 2,
    "comment__": "How often to update the training data and plots wrt # of rounds",
"plotting_update_freq_num_rounds": 2,
    "comment__": "How often to save the training data and plotting data",
"saving_update_freq_num_rounds": 2,
    "comment__": "Number of treads that can be run in parallel during training",
"num_available_threads": 5,
    "comment__": "Length of the queues used to pass simulation data between the simulation workers and the learning agent(s).",
"queue_size_limit": 50,
    "comment__": "Number of actions performed between training updates",
"sim_action_per_training_update": 8,
    "comment__": "Number of rounds of adaptive sampling",
"adaptive_samples": 5,
    "comment__": "Number of elite adaptive samples to keep between adaptive sampling rounds",
"num_adaptive_samples_to_keep": 50,
    "comment__": "Use the variance calculated from the policy network (calculated using dropout)",
"use_actor_policy_action_variance_suggestion": false,
    "comment__": "Method used for action exploration",
"exploration_method": "gaussian_random",
    "comment__": "Amount of dropout to use in the networks (if using a dropout network)",
"dropout_p": 0.1,
    "comment__": "Regularization weight for the policy network",
"regularization_weight": 0.001,
    "comment__": "Some parameter for rmsprop stocastic gradient optimization method.",
"rho": 0.95,
    "comment__": "Some parameter for rmsprop stocastic gradient optimization method.",
"rms_epsilon": 0.000001,
    "comment__": "Number of training updates before the target network is updated",
"steps_until_target_network_update": 500,
    "comment__": "Initial factor epsilon in multiplied by (This value will slowly be reduced during training)",
"epsilon_annealing": 0.8,
    "comment__": "Different ways of calculating the scaling method used normalize the input and outputs of the network from the bootstrapping samples. minmax, input and output are -mean/max-min. variance, input and output are -mean/(std*2), given, use the bounds provided in this file",
"state_normalization": "given",
    "comment__": "load a pretrained model for the controller",
"load_saved_model": false,
	"comment__": "Number of updates the critic should perform per actor update",
"critic_updates_per_actor_update": 5,
    "comment__": "weather or not to clamp actions to stay inside the action boundaries",
"clamp_actions_to_stay_inside_bounds": true,
    "comment__": "Number of initial actions to sample before calculating input/output scaling and starting to train.",
"bootstrap_samples": 1000,
    "comment__": "What method to use to select actions during bootstrapping",
"bootsrap_with_discrete_policy": true,
    "comment__": "That max number of action that can be take before the end of an episode/epoch",
"max_epoch_length": 100,
    "comment__": "If reward is below this bound it will not be put in the Experience Buffer",
"reward_lower_bound": -0.5,
    "comment__": "Enable guided policy search. Uses MCMC sampling ahead in time to select the best action to keep",
"use_guided_policy_search" : false,
    "comment__": "The number of training updates to perform for every action that is simulated",
"training_updates_per_sim_action": 1,
    "comment__": "Use The forward dynamics simulator as a way of sampling suggested actions for exploration",
"use_sampling_exploration": false,
    "comment__": "Use the forward dyanmics model to perform action exploration wrt to V -> fd > delta Action gradients",
"use_model_based_action_optimization": true,
    "comment__": "Flag for policy evaluation to swap in the task network from one model and the character/robot network from another",
"use_transfer_task_network": false,
    "comment__": "Add a large cost to actions that are suggested and are outside the action bounds.",
"penalize_actions_outside_bounds": false,
    "comment__": "Network type to use for the forward dynamics model",
"forward_dynamics_model_type": "model.ForwardDynamicsDenseNetworkDropout.ForwardDynamicsDenseNetworkDropout",
    "comment__": "Whether or not to save the Experience memory after bootstrapping",
"save_experience_memory": false,
    "comment__": "Whether or not to train the policy and critic?",
"train_rl_learning": true,
    "comment__": "Force the character to start each new action in a good state, close to a good solution",
"use_back_on_track_forcing": false,
    "comment__": "draw/render the next state suggested by the forward dynamics model",
"visualize_forward_dynamics": false,
    "comment__": "Learning rate for the forward dynamics model",
"fd_learning_rate": 0.001,
    "comment__": "Whether or not to train the policy. Used for debugging",
"train_actor": true,
    "comment__": "Plot the terms for the critic as well (regularization and td error)",
"debug_critic": false,
    "comment__": "critic regularization weight",
"critic_regularization_weight": 0.0001,
    "comment__": "Critic learning rate",
"critic_learning_rate": 0.0001,
    "comment__": "During evaluation plot of value function",
"visualize_expected_value": true,
    "comment__": "exponential decay value for use in reward function",
"target_velocity_decay":-0.75,
    "comment__": "Target velocity for controller",
"target_velocity":2.5,
    "comment__": "NUmmber of terrain features for which convolutional filters should be used",
"num_terrain_features": 100,
    "comment__": "Initial tempurature for annealing of e-greedy exploration",
"initial_temperature": 20.0,
    "comment__": "epsilon lower limit",
"min_epsilon": 0.15,
    "comment__": "Whether or not to draw/render the simulation",
"shouldRender": false,
    "comment__": "Learning rate use for model based action exploration",
"action_learning_rate": 10.0,
    "comment__": "During model-based action exloration, Probability of a random action being generated from MBAE,",
"model_based_action_omega": 0.0
}