{
"model_type": "Deep_CNN_Dropout",
"agent_name": "DeepQNetwork",
"data_folder": "Gaps_Capsule",
"epsilon": 0.35, 
"omega": 0.7,
"batch_size": 32,
"learning_rate": 0.001,
"anchor_file": "../data/really_easy_anchors.json",
"sim_config_file": "settings/ballgame1D/bouncegame_settings_capsule_gaps.json",
"forwardDynamics_config_file": "settings/ballgame1D/bouncegame_settings_capsule_gaps.json",
"num_actions": 9,
"exploration_rate": 0.2,
"rounds": 5000,
"epochs": 10,
"eval_epochs": 10,
"num_states": 10,
"discount_factor": 0.8,
"visualize_learning": true,
"save_trainData": true,
"train_forward_dynamics": false,
"visulaize_forward_dynamics": false,
"reward_bounds": [[0.0],[1.0]],
"expereince_length": 10000,
"state_bounds": [[-1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, 0.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0, -1.0],
                 [ 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.1, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]],
"action_bounds": [[0.5],
                  [3.5]],
"discrete_actions": [[1.12],
                    [2.02],
                    [3.1],
                    [2.87],
                    [1.4],
                    [2.2], 
                    [1.7],
                    [2.5],
                    [2.7]],                  
"action_space_continuous":false,
"train_on_validation_set":false,
"environment_type": "ballgame_1d",
"forward_dynamics_predictor": "simulator",
"sampling_method": "SequentialMC",
"use_actor_policy_action_suggestion": true,
"num_uniform_action_samples": 5,
"look_ahead_planning_steps": 2,
"plotting_update_freq_num_rounds": 5,
"saving_update_freq_num_rounds": 10,
"num_available_threads": 1,
"queue_size_limit": 100,
"sim_action_per_training_update": 32,
"adaptive_samples": 500,
"num_adaptive_samples_to_keep": 50,
"use_actor_policy_action_variance_suggestion": false,
"exploration_method": "uniform_random",
"dropout_p": 0.1,
"regularization_weight": 0.0001,
"rho": 0.95,
"rms_epsilon": 0.000001,
"steps_until_target_network_update": 1000,
"epsilon_annealing": 0.1,
"state_normalization": "variance",
    "comment__": "load a pretrained model for the controller",
"load_saved_model": false,
	"comment__": "Number of updates the critic should perform per actor update",
"critic_updates_per_actor_update": 5,
"clamp_actions_to_stay_inside_bounds": true,
"bootstrap_samples": 500,
"bootsrap_with_discrete_policy": true,
"max_epoch_length": 100,
"reward_lower_bound": -0.1,
"use_guided_policy_search" : false,
"training_updates_per_sim_action": 1,
"use_sampling_exploration": false,
"state_slice_point": 201
}