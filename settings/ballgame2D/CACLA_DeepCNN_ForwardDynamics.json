{
    "comment__": "Type of model/network to use for the actor and critic",
"model_type": "Deep_CNN",
    "comment__": "Learning algorithm to use",
"agent_name": "CACLA",
    "comment__": "Folder to store the training data in.",
"data_folder": "Gaps_Sphere_FD/",
"comment": "initial probability of selecting a random action",
"epsilon": 0.60, 
"comment": "initial probability of selecting a discrete random action",
"omega": 0.01,
    "comment__": "Batch size used for learning",
"batch_size": 32,
    "comment__": "Learning rate for the actor/policy",
"learning_rate": 0.0001,
    "comment__": "Config file for the simulator",
"sim_config_file": "./settings/ballgame2D/ballgame_settings_gaps.json",
    "comment__": "A different config file for the forward dynamics simulator",
"forwardDynamics_config_file": "./settings/ballgame2D/ballgame_settings_gaps.json",
    "comment__": "Exploration distance use when randomly generating new actions",
"exploration_rate": 0.15,
    "comment__": "Number of rounds to perform before termination",
"rounds": 5000,
    "comment__": "Number of epochs to perform per round",
"epochs": 10,
    "comment__": "Number of epoch/episode to evaluate the policy over",
"eval_epochs": 5,
    "comment__": "Discount factor used during learning",
"discount_factor": 0.95,
    "comment__": "Should the training be plotted during learning",
"visualize_learning": true,
    "comment__": "Whether or not to save the plotted data while learning",
"save_trainData": true,
    "comment__": "Whether or not to train a forward dynamics model as well",
"train_forward_dynamics": true,
    "comment__": "Whether or not to plot the training curve while learning for the forward dynamics model",
"visulaize_forward_dynamics": true,
    "comment__": "Bounds used for scaling rewards for networks",
"reward_bounds": [[0.0],[1.0]],
    "comment__": "Max length of the Experience memory",
"expereince_length": 10000,
    "comment__": "Possible state bounds to be used for scaling states for networks",
"state_bounds": [[ -0.32   , -0.49099545, -0.66236352, -0.80898154, -0.91016127,
		        -0.96159513, -0.94570464, -0.96682306, -1.01731105, -1.07166502,
		        -1.12032252, -1.14144528, -1.14972847, -1.12884363, -1.07166502,
		        -1.05318229, -1.07621798, -1.05318229, -1.07621798, -1.09416626,
		        -1.03664469, -0.94837491, -0.87004178, -0.80898154, -0.72167524,
		        -0.66621822, -0.68142988, -0.68142988, -0.6700519 , -0.68891553,
		        -0.6700519 , -0.73227843, -0.79939988, -0.85819997, -0.92676089,
		        -0.96159513, -0.99749609, -1.0318553 , -1.03664469, -1.05551667,
		        -1.04613723, -1.03902855, -1.06016465, -1.03664469, -1.05551667,
		        -1.02944969, -1.06016465, -1.05084095, -0.97974615, -0.94837491,
		        -0.88748201, -0.86709776, -0.83397406, -0.82157713, -0.81530463,
		        -0.76992307, -0.75984057, -0.76321624, -0.7461856 , -0.76992307,
		        -0.77325455, -0.83397406, -0.86414284, -0.92676089,  0.00446448,
		        -1.13000445, -1.35855331, -1.11149236, -0.77760577, -0.96229566,
		        -0.94504452, -0.92893317,  0.87701812],
		       [ 0.21      ,  0.38899545,  0.48236352,  0.54498154,  0.57816127,
		         0.59159513,  0.58770464,  0.59282306,  0.60331105,  0.61166502,
		         0.61632252,  0.61744528,  0.61772847,  0.61684363,  0.61166502,
		         0.60918229,  0.61221798,  0.60918229,  0.61221798,  0.61416626,
		         0.60664469,  0.58837491,  0.56604178,  0.54498154,  0.50967524,
		         0.48421822,  0.49142988,  0.49142988,  0.4860519 ,  0.49491553,
		         0.4860519 ,  0.51427843,  0.54139988,  0.56219997,  0.58276089,
		         0.59159513,  0.59949609,  0.6058553 ,  0.60664469,  0.60951667,
		         0.60813723,  0.60702855,  0.61016465,  0.60664469,  0.60951667,
		         0.60544969,  0.61016465,  0.60884095,  0.59574615,  0.58837491,
		         0.57148201,  0.56509776,  0.55397406,  0.54957713,  0.54730463,
		         0.52992307,  0.52584057,  0.52721624,  0.5201856 ,  0.52992307,
		         0.53125455,  0.55397406,  0.56414284,  0.58276089,  0.13444836,
		         1.05750561,  1.19799721,  1.01370458,  1.09383277,  1.05019649,
		         1.035661  ,  1.06712754,  3.26398188]],
    "comment__": "Action scaling values to be used to scale values for the network",
"action_bounds": [[-1.0, 2.5],
                  [1.0, 5.0]],
    "comment__": "Set of discrete actions that can be sampled from",				     
"discrete_actions": [[-0.92, 2.8],
                    [0.02, 3.0],
                    [0.3, 4.5],
                    [-0.4, 4.2],
                    [0.8, 4.9],
                    [-0.7, 3.0], 
                    [0.7, 4.6],
                    [-0.5, 2.6],
                    [-0.2, 3.6]],
    "comment__": "Is action space continuous or discrete?",
"action_space_continuous":true,
    "comment__": "Should the method train on the validation set only",
"train_on_validation_set":false,
    "comment__": "Name of the type of simulator to use",
"environment_type": "ballgame_2d",
    "comment__": "Model type to use for the forward dynamics model",
"forward_dynamics_predictor": "network",
    "comment__": "Method to be used for the forward dynamics model is the model types uses a simulator",
"sampling_method": "SequentialMC",
    "comment__": "Use the action suggested by the policy to start the sampling method.",
"use_actor_policy_action_suggestion": true,
    "comment__": "If selecting from a uniform distribution the number of regularly distant samples to take / action dimension",
"num_uniform_action_samples": 3,
    "comment__": "Number of steps ahead the actions should be sampled",
"look_ahead_planning_steps": 2,
    "comment__": "How often to update the training data and plots wrt # of rounds",
"plotting_update_freq_num_rounds": 5,
    "comment__": "How often to save the training data and plotting data",
"saving_update_freq_num_rounds": 5,
    "comment__": "Number of treads that can be run in parallel during training",
"num_available_threads": 3,
    "comment__": "Length of the queues used to pass simulation data between the simulation workers and the learning agent(s).",
"queue_size_limit": 50,
    "comment__": "Number of actions performed between training updates",
"sim_action_per_training_update": 8,
    "comment__": "Number of rounds of adaptive sampling",
"adaptive_samples": 5,
    "comment__": "Number of elite adaptive samples to keep between adaptive sampling rounds",
"num_adaptive_samples_to_keep": 50,
    "comment__": "Use the variance calculated from the policy network (calculated using dropout)",
"use_actor_policy_action_variance_suggestion": false,
    "comment__": "Method used for action exploration",
"exploration_method": "gaussian_random",
    "comment__": "Amount of dropout to use in the networks (if using a dropout network)",
"dropout_p": 0.1,
    "comment__": "Regularization weight for the policy network",
"regularization_weight": 0.00001,
    "comment__": "Some parameter for rmsprop stocastic gradient optimization method.",
"rho": 0.95,
    "comment__": "Some parameter for rmsprop stocastic gradient optimization method.",
"rms_epsilon": 0.000001,
    "comment__": "Number of training updates before the target network is updated",
"steps_until_target_network_update": 500,
    "comment__": "Initial factor epsilon in multiplied by (This value will slowly be reduced during training)",
"epsilon_annealing": 0.8,
    "comment__": "Different ways of calculating the scaling method used normalize the input and outputs of the network from the bootstrapping samples. minmax, input and output are -mean/max-min. variance, input and output are -mean/(std*2), given, use the bounds provided in this file",
"state_normalization": "given",
    "comment__": "load a pretrained model for the controller",
"load_saved_model": false,
	"comment__": "Number of updates the critic should perform per actor update",
"critic_updates_per_actor_update": 2,
    "comment__": "weather or not to clamp actions to stay inside the action boundaries",
"clamp_actions_to_stay_inside_bounds": false,
    "comment__": "Number of initial actions to sample before calculating input/output scaling and starting to train.",
"bootstrap_samples": 1000,
    "comment__": "What method to use to select actions during bootstrapping",
"bootsrap_with_discrete_policy": true,
    "comment__": "That max number of action that can be take before the end of an episode/epoch",
"max_epoch_length": 100,
    "comment__": "If reward is below this bound it will not be put in the Experience Buffer",
"reward_lower_bound": -0.5,
    "comment__": "Enable guided policy search. Uses MCMC sampling ahead in time to select the best action to keep",
"use_guided_policy_search" : false,
    "comment__": "The number of training updates to perform for every action that is simulated",
"training_updates_per_sim_action": 1,
    "comment__": "Use The forward dynamics simulator as a way of sampling suggested actions for exploration",
"use_sampling_exploration": false,
    "comment__": "Use the forward dyanmics model to perform action exploration wrt to V -> fd > delta Action gradients",
"use_model_based_action_optimization": true,
    "comment__": "Flag for policy evaluation to swap in the task network from one model and the character/robot network from another",
"use_transfer_task_network": false,
    "comment__": "Add a large cost to actions that are suggested to be outside the action bounds.",
"penalize_actions_outside_bounds": false,
    "comment__": "Network type to use for the forward dynamics model",
"forward_dynamics_model_type": "Deep_CNN",
    "comment__": "Whether or not to save the Experience memory after bootstrapping",
"save_experience_memory": true,
    "comment__": "Whether or not to train the policy and critic?",
"train_rl_learning": true,
    "comment__": "Force the character to start each new action in a good state, close to a good solution",
"use_back_on_track_forcing": false,
    "comment__": "draw/render the next state suggested by the forward dynamics model",
"visualize_forward_dynamics": false,
    "comment__": "Learning rate for the forward dynamics model",
"fd_learning_rate": 0.001,
    "comment__": "Whether or not to train the policy. Used for debugging",
"train_actor": true,
    "comment__": "Plot the terms for the critic as well (regularization and td error), Warning: might use a lot of memory",
"debug_critic": true,
    "comment__": "critic regularization weight",
"critic_regularization_weight": 0.000001,
    "comment__": "Critic learning rate",
"critic_learning_rate": 0.001,
    "comment__": "During evaluation plot of value function",
"visualize_expected_value": true,
    "comment__": "exponential decay value for use in reward function",
"target_velocity_decay":-0.75,
    "comment__": "Target velocity for controller",
"target_velocity":2.25,
    "comment__": "NUmmber of terrain features for which convolutinoal filters should be used",
"num_terrain_features": 64,
    "comment__": "Initial tempurature for annealing of e-greedy exploration",
"initial_temperature": 15.0,
    "comment__": "epsilon lower limit",
"min_epsilon": 0.15,
    "comment__": "Whether or not to draw/render the simulation",
"shouldRender": false,
    "comment__": "Learning rate use for model based action exploration",
"action_learning_rate": 0.001,
    "comment__": "During model-based action exploration, initial probability of performing model-based opt action",
"model_based_action_omega": 0.25,
	"comment__": "visualize the loss and regularization cost of the actor. Warning: might use a lot of memory",
"debug_actor": true,
	"comment__": "float type to use, if for example you want to train on the GPU use float32",
"float_type": "float64",
	"comment__": "What processor type to perform the training on [cpu|gpu]",
"training_processor_type": "cpu"
}
