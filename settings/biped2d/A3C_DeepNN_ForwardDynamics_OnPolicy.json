{
"model_type": "Deep_NN",
"agent_name": "A3C",
"data_folder": "Simple_Walk_FD_Dyna_Reg_OnPolicy/",
"epsilon": 0.5, 
"omega": 0.0,
"batch_size": 32,
"learning_rate": 0.001,
"sim_config_file": "../data/simbiconBiped2D/trainContinuousActions.ini",
"forwardDynamics_config_file": "../data/simbiconBiped2D/trainContinuousActions.ini",
"num_actions": 9,
"exploration_rate": 0.1,
"rounds": 10000,
"epochs": 10,
"eval_epochs": 10,
"discount_factor": 0.8,
"visualize_learning": true,
"save_trainData": true,
"train_forward_dynamics": true,
"reward_bounds": [[0.0],[1.0]],
"expereince_length": 1000,
"state_bounds": [[-3.14, 1.0,-8.05, -1.0, -1.0, -8.5, -5.0, -5.5, -5.0, -2.5, -3.14],
				 [ 3.14, 8.5, 5.00,  1.0,  1.0, 10.0,  5.0, 15.0,  5.0, 20.0,  3.14]],
"comment__": "Action  phase, root forward,  swing hip sagital,                 feedback cd  cv,     coronal,   swing knee z,                    stance hip,         stance knee,     swing ankle x,        stance ankle x,      feedback cd cv,  stance shoulder,        swing shoulder,              stance elbow,                     swing elbow,                    pelvis_torso ",
"action_bounds":   [[ 0.49,  -0.25, -0.25,  -0.75,      -0.75,     -0.75,     -0.75,      -0.75,    -0.3,     -0.1,     -0.1,     -0.1,        -0.75,-0.75, -0.75, -0.3,     -0.3,  -1.5,      -1.5,      -1.5,     -1.5,       0.0,     0.0,   -2.25, -2.25,  -2.25,   -2.25,    -2.25,  -2.25,     -0.25,    -0.25,     -0.25,       -0.25,     -0.25,     -0.25,    -0.7   ],
				     [0.51,   0.25,  0.25,   0.75,       0.75,      0.75,      0.5,       0.5,       0.3,      1.75,     1.75,     1.75,        0.75, 0.75,  0.75,  1.3,      1.3,   2.0,       2.0,       2.0,      2.0,       0.4,     0.4,    2.25,  2.25,   2.25,    2.25,     2.25,   2.25,      4.25,     4.25,      4.25,        4.25,      4.25,      4.25,     0.7   ]],
"discrete_actions": [[0.5,   0.06, 0.03,    -0.5500000, -0.366476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,   0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.6,  -0.16, 0.074,   -0.4500000, -0.266476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,   0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.55,  0.06, 0.14,    -0.500000,  -0.216476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,   0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
	             	 [0.45, -0.8, -0.045,   -0.500000,  -0.316476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.65, -0.08, 0.155,   -0.500000,  -0.266476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
	             	 [0.55,  0.08, 0.045,   -0.4500000, -0.266476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.5,  -0.1,  0.05,    -0.500000,  -0.266476, -0.103272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
         			 [0.45,  0.1, -0.165,   -0.400000,  -0.266476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.61,  0.1,  0.2,     -0.500000,  -0.266476, -0.093272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0]],	  
"action_space_continuous":true,
"train_on_validation_set":true,
"environment_type": "simbiconBiped2D",
"forward_dynamics_predictor": "network",
"sampling_method": "SequentialMC",
"use_actor_policy_action_suggestion": true,
"num_uniform_action_samples": 3,
"look_ahead_planning_steps": 2,
"plotting_update_freq_num_rounds": 10,
"saving_update_freq_num_rounds": 10,
"num_available_threads": 14,
"queue_size_limit": 100,
"sim_action_per_training_update": 8,
"adaptive_samples": 5,
"num_adaptive_samples_to_keep": 50,
"use_actor_policy_action_variance_suggestion": false,
"exploration_method": "gaussian_random",
"dropout_p": 0.1,
"regularization_weight": 0.00001,
"rho": 0.95,
"rms_epsilon": 0.000001,
"steps_until_target_network_update": 1000,
"epsilon_annealing": 0.8,
"state_normalization": "variance",
    "comment__": "load a pretrained model for the controller",
"load_saved_model": false,
	"comment__": "Number of updates the critic should perform per actor update",
"critic_updates_per_actor_update": 1,
"clamp_actions_to_stay_inside_bounds": true,
"bootstrap_samples": 1000,
"bootsrap_with_discrete_policy": true,
"max_epoch_length": 100,
"reward_lower_bound": -0.5,
"use_guided_policy_search" : false,
"training_updates_per_sim_action": 1,
"use_sampling_exploration": false,
"use_model_based_action_optimization": true,
"use_transfer_task_network": false,
"penalize_actions_outside_bounds": false,
"use_transfer_task_network": false,
"forward_dynamics_model_type": "model.ForwardDynamicsDenseNetworkDropout.ForwardDynamicsDenseNetworkDropout",
"save_experience_memory": false,
"train_rl_learning": true,
"use_back_on_track_forcing": false,
"visualize_forward_dynamics": false,
"fd_learning_rate": 0.01,
"train_actor": true,
"debug_critic": true,
"critic_regularization_weight": 0.000001,
"critic_learning_rate": 0.01,
"visualize_expected_value": true,
"target_velocity_decay":-2.0,
"target_velocity":1.0,
"num_terrain_features": 0,
"initial_temperature": 5.0,
"min_epsilon": 0.15,
"shouldRender": false,
    "comment__": "Learning rate use for model based action exploration",
"action_learning_rate": 0.5,
    "comment__": "During model-based action exploration, Probability of model-based action policy action",
"model_based_action_omega": 0.2,
	"comment__": "visualize the loss and regularization cost of the actor. Warning: might use a lot of memory",
"debug_actor": true,
	"comment__": "float type to use, if for example you want to train on the GPU use float32",
"float_type": "float64",
	"comment__": "What processor type to perform the training on [cpu|gpu]",
"training_processor_type": "cpu",
	"comment__": "optimizer is the type of optimization algorithm to use",
"optimizer": "adam",
	"comment__": "This setting uses a sampling method overtop of simulation",
"use_simulation_sampling": false,
	"comment__": "Variance scaling used for sampling",
"variance_scalling": 0.1,
	"comment__": "Whether or not to parameterize the control via the reward function, also adds parameters to input state",
"use_parameterized_control": false,
"controller_parameter_settings": {
		"velocity_bounds": [[0.5],[2.5]],
		"root_height_bounds": [[0.7],[1.1]],
		"root_pitch_bounds": [[-0.25],[0.25]],
		"right_hand_x_pos_bounds": [[0.0],[0.5]],
		"torque_bounds": [[0.0],[0.5]]
		},
	"comment__": "THe parameter used to control the average change in the control parameters",
"average_parameter_change": 0.2,
	"comment__": "Whether or not to train the value function some output from the forward dynamics",
"train_critic_on_fd_output": true,
	"comment__": "Use to add an additional regularization term to prevent the network from moving to far from its previous values",
"use_previous_value_regularization": true,
	"comment__": "Controls the level of information that is printed to the terminal",
"print_level": "train",
	"comment__": "print level descriptions",
"print_levels": {
		"debug": 1,
		"train": 0,
		"hyper_train": -1
		},
	"comment__": "The type of function to apply over the controller target values [gaussian|abs]",
"reward_smoother": "gaussian",
	"comment__": "Weights for different components of the reward function",
"controller_reward_weights": {
		"velocity": 0.8,
		"torque": 0.05,
		"root_height": 0.05,
		"root_pitch": 0.1,
		"right_hand_x_pos": 0.0
		},
	"comment__": "Regularization weight for different between policy parameters and old policy parameters",
"previous_value_regularization_weight":  0.005,
	"comment__": "Random seed value for the simulation to use",
"random_seed": 1234
}
