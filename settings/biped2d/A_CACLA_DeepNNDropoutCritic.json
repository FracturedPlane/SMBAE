{
    "comment__": "Type of model/network to use for the actor and critic",
"model_type": "Deep_NN_Dropout_Critic",
    "comment__": "Learning algorithm to use",
"agent_name": "A_CACLA",
    "comment__": "Folder to store the training data in.",
"data_folder": "Simple_walk/",
	"comment": "initial probability of selecting a random action",
"epsilon": 0.55, 
	"comment": "initial probability of selecting a discrete random action",
"omega": 0.15,
    "comment__": "Batch size used for learning",
"batch_size": 32,
    "comment__": "Learning rate for the actor/policy",
"learning_rate": 0.0001,
    "comment__": "Config file for the simulator",
"sim_config_file": "../data/simbiconBiped2D/trainContinuousActions.ini",
    "comment__": "A different config file for the forward dynamics simulator",
"forwardDynamics_config_file": "../data/simbiconBiped2D/trainContinuousActions.ini",
    "comment__": "Exploration rate use when randomly generating new actions",
"exploration_rate": 0.15,
    "comment__": "Number of rounds to perform before termination",
"rounds": 500000,
    "comment__": "Number of epochs to perform per round",
"epochs": 10,
    "comment__": "Number of epoch/episode to evaluate the policy over",
"eval_epochs": 10,
    "comment__": "Discount factor used during learning",
"discount_factor": 0.95,
    "comment__": "Should the training be plotted during learning",
"visualize_learning": true,
    "comment__": "Whether or not to save the plotted data while learning",
"save_trainData": true,
    "comment__": "Whether or not to train a forward dynamics model as well",
"train_forward_dynamics": false,
    "comment__": "Bounds used for scaling rewards for networks",
"reward_bounds": [[0.0],[1.0]],
    "comment__": "Max length of the Experience memory",
"expereince_length": 10000,
    "comment__": "Possible state bounds to be used for scaling states for networks",
"state_bounds": [[0.79227799, -3.51146007,  -0.8932817 ,  -0.8292731 ,
	         -3.24586916,  -0.12381294,   0.25702363,  -0.59987533,
	         -0.81159002,  -0.2411634 ,  -4.75630379,  -0.11473341,
	         -0.31742597,  -0.95632511,  -0.44578609,  -0.71053219,
	         -7.69628143,  -0.10928281,  -0.31042406,  -0.92769933,
	         -0.4685356 ,  -0.58194721,  -7.43715429,  -0.15127681,
	          0.41375017,  -0.88155276,  -1.3530277 ,  -0.57746065,
	         -7.42206144,  -0.08162763,   0.16980466,  -0.61480427,
	         -0.66983718,  -3.44586134,  -3.65565228,  -0.0823019 ,
	          0.18377568,  -0.61387658,  -0.68719655,  -3.10081434,
	         -3.45911002,  -0.35725731,  -0.88959008,  -1.7191509 ,
	         -1.03030622,  -0.917974  ,  -6.3901329 ,  -0.34188899,
	         -0.87650335,  -1.69257486,  -1.04027009,  -0.85638893,
	         -6.59318495,  -0.05210859,  -0.15308192,  -0.87337536,
	         -0.29142395,  -3.27884984,  -4.9951601 ,  -0.05089432,
	         -0.13649182,  -0.88233888,  -0.25719821,  -3.51509738,
	         -4.96978092,  -0.46915358,  -1.22309649,  -1.87023997,
	         -1.16855919,  -0.6669237 ,  -6.29674339,  -0.44613662,
	         -1.20296133,  -1.85118794,  -1.0835067 ,  -0.59030527,
	         -7.28811884,  -0.41062886,  -1.23562849,  -1.82952428,
	         -0.9187994 ,  -0.57888949,  -8.06137276,  -0.37985   ,
	         -1.21550822,  -1.81209254,  -0.83225882,  -0.48842043,
	         -9.14271736],
				 [ 1.18387926,   1.66575396,   0.63065016,   1.61849594,
				 4.18461275,   0.14270517,   0.51769435,   0.47771817,
	          0.72500408,   0.11040128,   3.40354562,   0.13009316,
	         -0.18212497,   0.7071119 ,   0.30586734,   0.87731177,
	          5.07596493,   0.12714678,  -0.18246856,   0.74448448,
	          0.34457716,   0.86146623,   5.24312401,   0.30632678,
	          0.84221137,   0.847983  ,   1.13917828,   0.24148114,
	          5.60893202,   0.12073349,   0.39001611,   0.48501632,
	          0.58363879,   1.49184811,   2.52697754,   0.12754926,
	          0.40491089,   0.47194228,   0.59487838,   0.54106617,
	          2.31779623,   0.30343184,  -0.49257278,   1.24272788,
	          0.71459156,   0.48656762,   5.98726225,   0.3053517 ,
	         -0.50473642,   1.37695181,   0.76716191,   0.56802279,
	          6.20354986,   0.04702853,  -0.05196139,   0.6758486 ,
	          0.25732684,   3.01113939,   3.75101089,   0.05117951,
	         -0.03982695,   0.63778108,   0.21364087,   2.281533  ,
	          3.59415889,   0.39437586,  -0.67277986,   1.65084136,
	          0.96368027,   0.62217236,   8.4931469 ,   0.40397301,
	         -0.6953941 ,   1.78102195,   0.91787279,   0.62518257,
	          9.01820946,   0.47600806,  -0.70763177,   1.71049118,
	          1.00366211,   0.69063246,  12.93024826,   0.47936273,
	         -0.72998667,   1.81443799,   0.92729098,   0.67199892,
	         13.51691723]],
"comment__": "Action  phase, root forward,  swing hip sagital,                 feedback cd  cv,     coronal,   swing knee z,                    stance hip,         stance knee,     swing ankle x,        stance ankle x,      feedback cd cv,  stance shoulder,        swing shoulder,              stance elbow,                     swing elbow,                    pelvis_torso ",
"action_bounds":   [[ 0.49,  -0.25, -0.25,  -0.75,      -0.75,     -0.75,     -0.75,      -0.75,    -0.3,     -0.1,     -0.1,     -0.1,        -0.75,-0.75, -0.75, -0.3,     -0.3,  -1.5,      -1.5,      -1.5,     -1.5,       0.0,     0.0,   -2.25, -2.25,  -2.25,   -2.25,    -2.25,  -2.25,     -0.25,    -0.25,     -0.25,       -0.25,     -0.25,     -0.25,    -0.7   ],
				     [0.51,   0.25,  0.25,   0.75,       0.75,      0.75,      0.5,       0.5,       0.3,      1.75,     1.75,     1.75,        0.75, 0.75,  0.75,  1.3,      1.3,   2.0,       2.0,       2.0,      2.0,       0.4,     0.4,    2.25,  2.25,   2.25,    2.25,     2.25,   2.25,      4.25,     4.25,      4.25,        4.25,      4.25,      4.25,     0.7   ]],
"discrete_actions": [[0.5,   0.06, 0.03,    -0.5500000, -0.366476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,   0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.6,  -0.16, 0.074,   -0.4500000, -0.266476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,   0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.55,  0.06, 0.14,    -0.500000,  -0.216476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,   0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
	             	 [0.45, -0.8, -0.045,   -0.500000,  -0.316476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.65, -0.08, 0.155,   -0.500000,  -0.266476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
	             	 [0.55,  0.08, 0.045,   -0.4500000, -0.266476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.5,  -0.1,  0.05,    -0.500000,  -0.266476, -0.103272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
         			 [0.45,  0.1, -0.165,   -0.400000,  -0.266476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.61,  0.1,  0.2,     -0.500000,  -0.266476, -0.093272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0]],
    "comment__": "Is action space continuous or discrete?",
"action_space_continuous":true,
    "comment__": "Should the method train on the validation set only",
"train_on_validation_set":true,
    "comment__": "Name of the type of simulator to use",
"environment_type": "simbiconBiped2D",
    "comment__": "Model type to use for the forward dynamics model",
"forward_dynamics_predictor": "network",
    "comment__": "Method to be used for the forward dynamics model is the model types uses a simulator",
"sampling_method": "SequentialMC",
    "comment__": "Use the action suggested by the policy to start the sampling method.",
"use_actor_policy_action_suggestion": true,
    "comment__": "If selecting from a uniform distribution the number of regularly distant samples to take / action dimension",
"num_uniform_action_samples": 3,
    "comment__": "Number of steps ahead the actions should be sampled",
"look_ahead_planning_steps": 2,
    "comment__": "How often to update the training data and plots wrt # of rounds",
"plotting_update_freq_num_rounds": 10,
    "comment__": "How often to save the training data and plotting data",
"saving_update_freq_num_rounds": 10,
    "comment__": "Number of treads that can be run in parallel during training",
"num_available_threads": 10,
    "comment__": "Length of the queues used to pass simulation data between the simulation workers and the learning agent(s).",
"queue_size_limit": 1000,
    "comment__": "Number of actions performed between training updates",
"sim_action_per_training_update": 8,
    "comment__": "Number of rounds of adaptive sampling",
"adaptive_samples": 5,
    "comment__": "Number of elite adaptive samples to keep between adaptive sampling rounds",
"num_adaptive_samples_to_keep": 50,
    "comment__": "Use the variance calculated from the policy network (calculated using dropout)",
"use_actor_policy_action_variance_suggestion": false,
    "comment__": "Method used for action exploration",
"exploration_method": "gaussian_random",
    "comment__": "Amount of dropout to use in the networks (if using a dropout network)",
"dropout_p": 0.1,
    "comment__": "Regularization weight for the policy network",
"regularization_weight": 0.000001,
    "comment__": "Some parameter for rmsprop stochastic gradient optimization method.",
"rho": 0.95,
    "comment__": "Some parameter for rmsprop stochastic gradient optimization method.",
"rms_epsilon": 0.000001,
    "comment__": "Number of training updates before the target network is updated",
"steps_until_target_network_update": 500,
    "comment__": "Initial factor epsilon in multiplied by (This value will slowly be reduced during training)",
"epsilon_annealing": 0.8,
    "comment__": "Different ways of calculating the scaling method used normalize the input and outputs of the network from the bootstrapping samples. minmax, input and output are -mean/max-min. variance, input and output are -mean/(std*2), given, use the bounds provided in this file",
"state_normalization": "variance",
    "comment__": "load a pretrained model for the controller",
"load_saved_model": false,
	"comment__": "Number of updates the critic should perform per actor update",
"critic_updates_per_actor_update": 1,
    "comment__": "weather or not to clamp actions to stay inside the action boundaries",
"clamp_actions_to_stay_inside_bounds": true,
    "comment__": "Number of initial actions to sample before calculating input/output scaling and starting to train.",
"bootstrap_samples": 2500,
    "comment__": "What method to use to select actions during bootstrapping",
"bootsrap_with_discrete_policy": true,
    "comment__": "That max number of action that can be take before the end of an episode/epoch",
"max_epoch_length": 100,
    "comment__": "If reward is below this bound it will not be put in the Experience Buffer",
"reward_lower_bound": -0.5,
    "comment__": "Enable guided policy search. Uses MCMC sampling ahead in time to select the best action to keep",
"use_guided_policy_search" : false,
    "comment__": "The number of training updates to perform for every action that is simulated",
"training_updates_per_sim_action": 1,
    "comment__": "Use The forward dynamics simulator as a way of sampling suggested actions for exploration",
"use_sampling_exploration": false,
    "comment__": "Use the forward dyanmics model to perform action exploration wrt to V -> fd > delta Action gradients",
"use_model_based_action_optimization": true,
    "comment__": "Flag for policy evaluation to swap in the task network from one model and the character/robot network from another",
"use_transfer_task_network": false,
    "comment__": "Add a large cost to actions that are suggested to be outside the action bounds.",
"penalize_actions_outside_bounds": false,
    "comment__": "Network type to use for the forward dynamics model",
"forward_dynamics_model_type": "model.ForwardDynamicsDenseNetworkDropout.ForwardDynamicsDenseNetworkDropout",
    "comment__": "Whether or not to save the Experience memory after bootstrapping",
"save_experience_memory": false,
    "comment__": "Whether or not to train the policy and critic?",
"train_rl_learning": true,
    "comment__": "Force the character to start each new action in a good state, close to a good solution",
"use_back_on_track_forcing": false,
    "comment__": "draw/render the next state suggested by the forward dynamics model",
"visualize_forward_dynamics": false,
    "comment__": "Learning rate for the forward dynamics model",
"fd_learning_rate": 0.01,
    "comment__": "Whether or not to train the policy. Used for debugging",
"train_actor": true,
    "comment__": "Plot the terms for the critic as well (regularization and td error)",
"debug_critic": true,
    "comment__": "critic regularization weight",
"critic_regularization_weight": 0.000001,
    "comment__": "Critic learning rate",
"critic_learning_rate": 0.001,
    "comment__": "During evaluation plot of value function",
"visualize_expected_value": true,
    "comment__": "exponential decay value for use in reward function",
"target_velocity_decay":-2.5,
    "comment__": "Target velocity for controller",
"target_velocity":1.5,
    "comment__": "NUmmber of terrain features for which convolutinoal filters should be used",
"num_terrain_features": 0,
    "comment__": "Initial tempurature for annealing of e-greedy exploration",
"initial_temperature": 2.5,
    "comment__": "epsilon lower limit",
"min_epsilon": 0.15,
    "comment__": "Whether or not to draw/render the simulation",
"shouldRender": false,
    "comment__": "Learning rate use for model based action exploration",
"action_learning_rate": 1.0,
    "comment__": "During model-based action exloration, Probability of a random action being generated from MBAE,",
"model_based_action_omega": 0.0,
	"comment__": "visualize the loss and regularization cost of the actor. Warning: might use a lot of memory",
"debug_actor": true,
	"comment__": "float type to use, if for example you want to train on the GPU use float32",
"float_type": "float64",
	"comment__": "What processor type to perform the training on [cpu|gpu]",
"training_processor_type": "cpu",
	"comment__": "optimizer is the type of optimization algorithm to use",
"optimizer": "adam",
	"comment__": "This setting uses a sampling method overtop of simulation",
"use_simulation_sampling": false,
	"comment__": "Variance scaling used for sampling",
"variance_scalling": 0.1,
	"comment__": "Whether or not to parameterize the control via the reward function, also adds parameters to input state",
"use_parameterized_control": false,
	"comment__": "parameter bounds for parameterized controller",
"controller_parameter_settings": {
		"velocity_bounds": [[0.5],[2.5]],
		"root_height_bounds": [[0.7],[1.1]],
		"root_pitch_bounds": [[-0.25],[0.25]],
		"right_hand_x_pos_bounds": [[0.0],[0.5]],
		"torque_bounds": [[0.0],[1.0]]
		},
	"comment__": "The parameter used to control the average change in the control parameters",
"average_parameter_change": 0.25,
	"comment__": "Whether or not to train the value function some output from the forward dynamics",
"train_critic_on_fd_output": false,
	"comment__": "Use to add an additional regularization term to prevent the network from moving to far from its previous values",
"use_previous_value_regularization": false,
	"comment__": "Controls the level of information that is printed to the terminal",
"print_level": "train",
	"comment__": "print level descriptions",
"print_levels": {
		"debug": 1,
		"train": 0,
		"hyper_train": -1
		},
	"comment__": "The type of function to apply over the controller target values [gaussian|abs]",
"reward_smoother": "abs",
	"comment__": "Weights for different components of the reward function",
"controller_reward_weights": {
		"velocity": 0.8,
		"torque": 0.05,
		"root_height": 0.1,
		"root_pitch": 0.05,
		"right_hand_x_pos": 0.0
		},
	"comment__": "Regularization weight for different between policy parameters and old policy parameters",
"previous_value_regularization_weight":  0.001,
	"comment__": "Random seed value for the simulation to use",
"random_seed": 1234,
	"comment__": "Makes a few changes to the flow of control in order for things to be on policy",
"on_policy": false,
	"comment__": "Whether or not to use a stochastic policy, This adds more outputs to the network and changes the way actions are sampled",
"use_stocastic_policy": false,
	"comment__": "Whether or  not to train the critic at all. Usually used for debugging",
"train_critic": true,
	"comment__": "What type of regularization to use",
"regularization_type": "kl",
	"comment__": "Whether or not to collects tuples in batches, this can be good for multi-threading or computing furture discounted reward",
"collect_tuples_in_batches":false,
	"comment__": "Whether or not the controller should be reset to a new epoch when a fall (fallen into some kind of non-recoverable state) has occured",
"reset_on_fall":true,
	"comment__": "Whether a model of the reward r <- R(s,a) should be trained",
"train_reward_predictor": true,
	"comment__": "How many gradient steps model based actino exploration should take",
"num_mbae_steps": 1,
    "comment__": "Whether or not the actor buffer should be fixed to process batches of a certain size",
"fix_actor_batch_size": false
}