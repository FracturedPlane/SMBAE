{
"model_type": "Deep_NN",
"agent_name": "CACLA",
"data_folder": "Simple_Walk/",
"epsilon": 0.5, 
"omega": 0.15,
"batch_size": 32,
"learning_rate": 0.001,
"anchor_file": "../data/anchorData/paperGibbonAnchors.json",
"sim_config_file": "../data/simbiconBiped2D/trainContinuousActions.ini",
"forwardDynamics_config_file": "../data/simbiconBiped2D/trainContinuousActions.ini",
"num_actions": 9,
"exploration_rate": 0.1,
"rounds": 100000,
"epochs": 10,
"eval_epochs": 10,
"discount_factor": 0.8,
"visualize_learning": true,
"save_trainData": true,
"train_forward_dynamics": false,
"reward_bounds": [[0.0],[1.0]],
"expereince_length": 10000,
"state_bounds": [[-3.14, 1.0,-8.05, -1.0, -1.0, -8.5, -5.0, -5.5, -5.0, -2.5, -3.14],
				 [ 3.14, 8.5, 5.00,  1.0,  1.0, 10.0,  5.0, 15.0,  5.0, 20.0,  3.14]],
"comment__": "Action  phase, root forward,  swing hip sagital,                 coronal,  swing knee z,               ",
"action_bounds":   [[ 0.49,  -0.25, -0.25,     -0.6,       -0.6,      -0.6,      -0.3,     -0.1,     -0.1,     -0.1     ],
				     [0.51,   0.25,  0.25,      0.6,        0.6,       0.6,       0.3,      1.5,      1.5,      1.5    ]],
"discrete_actions": [[0.5,   0.06, 0.03,    -0.5500000, -0.366476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.6,  -0.16, 0.074,   -0.4500000, -0.266476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.55,  0.06, 0.14,    -0.500000,  -0.216476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.45, -0.8, -0.045,   -0.500000,  -0.316476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.65, -0.08, 0.155,   -0.500000,  -0.266476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.55,  0.08, 0.045,   -0.4500000, -0.266476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.5,  -0.1,  0.05,    -0.500000,  -0.266476, -0.103272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.45,  0.1, -0.165,   -0.400000,  -0.266476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.61,  0.1,  0.2,     -0.500000,  -0.266476, -0.093272, -0.084616, 0.292735, 1.180385, 0.080433]],	  
"action_space_continuous":true,
"train_on_validation_set":true,
"environment_type": "simbiconBiped2D",
"forward_dynamics_predictor": "network",
"sampling_method": "SequentialMC",
"use_actor_policy_action_suggestion": true,
"num_uniform_action_samples": 3,
"look_ahead_planning_steps": 2,
"plotting_update_freq_num_rounds": 10,
"saving_update_freq_num_rounds": 10,
"num_available_threads": 12,
"queue_size_limit": 100,
"sim_action_per_training_update": 8,
"adaptive_samples": 5,
"num_adaptive_samples_to_keep": 50,
"use_actor_policy_action_variance_suggestion": false,
"exploration_method": "gaussian_random",
"dropout_p": 0.1,
"regularization_weight": 0.00001,
"rho": 0.95,
"rms_epsilon": 0.000001,
"steps_until_target_network_update": 1000,
"epsilon_annealing": 0.8,
"state_normalization": "variance",
    "comment__": "load a pretrained model for the controller",
"load_saved_model": false,
	"comment__": "Number of updates the critic should perform per actor update",
"critic_updates_per_actor_update": 2,
"clamp_actions_to_stay_inside_bounds": true,
"bootstrap_samples": 1000,
"bootsrap_with_discrete_policy": true,
"max_epoch_length": 100,
"reward_lower_bound": -0.5,
"use_guided_policy_search" : false,
"training_updates_per_sim_action": 1,
"use_sampling_exploration": false,
"use_model_based_action_optimization": true,
"use_transfer_task_network": false,
"penalize_actions_outside_bounds": false,
"use_transfer_task_network": false,
"forward_dynamics_model_type": "model.ForwardDynamicsDenseNetworkDropout.ForwardDynamicsDenseNetworkDropout",
"save_experience_memory": false,
"train_rl_learning": true,
"use_back_on_track_forcing": false,
"visualize_forward_dynamics": false,
"fd_learning_rate": 0.01,
"train_actor": true,
"debug_critic": true,
"critic_regularization_weight": 0.000001,
"critic_learning_rate": 0.01,
"visualize_expected_value": true,
"target_velocity_decay":-2.0,
"target_velocity":1.0,
"num_terrain_features": 0,
"initial_temperature": 5.0,
"min_epsilon": 0.15,
"shouldRender": false,
    "comment__": "Learning rate use for model based action exploration",
"action_learning_rate": 10.0,
    "comment__": "During model-based action exloration, Probability of a random action being generated from MBAE,",
"model_based_action_omega": 0.0,
	"comment__": "visualize the loss and regularization cost of the actor. Warning: might use a lot of memory",
"debug_actor": true,
	"comment__": "float type to use, if for example you want to train on the GPU use float32",
"float_type": "float64",
	"comment__": "What processor type to perform the training on [cpu|gpu]",
"training_processor_type": "cpu",
	"comment__": "optimizer is the type of optimization algorithm to use",
"optimizer": "adam",
	"comment__": "This setting uses a sampling method overtop of simulation",
"use_simulation_sampling": false,
	"comment__": "Variance scaling used for sampling",
"variance_scalling": 0.1,
	"comment__": "Whether or not to parameterize the control via the reward function, also adds parameters to input state",
"use_parameterized_control": false,
	"comment__": "parameter bounds for parameterized controller",
"controller_parameter_settings": {
		"velocity_bounds": [[0.5],[2.5]],
		"root_height_bounds": [[0.7],[1.1]],
		"root_pitch_bounds": [[-0.25],[0.25]]
		}
}
