{
"model_type": "Deep_CNN",
"agent_name": "A_CACLA",
"data_folder": "OnPolicy_Simple_Walk_Gaps/",
"epsilon": 0.5, 
"omega": 0.15,
"batch_size": 32,
"learning_rate": 0.001,
"sim_config_file": "../data/simbiconBiped2D/trainbiped2DTerrain.ini",
"forwardDynamics_config_file": "../data/simbiconBiped2D/trainbiped2DTerrain.ini",
"num_actions": 9,
"exploration_rate": 0.1,
"rounds": 100000,
"epochs": 10,
"eval_epochs": 10,
"discount_factor": 0.8,
"visualize_learning": true,
"save_trainData": true,
"train_forward_dynamics": false,
"reward_bounds": [[0.0],[1.0]],
"expereince_length": 10000,
"state_bounds": [[  0.83837616,   0.81930584,   0.80861211,   0.78828681,
          0.76718044,   0.75275195,   0.73964351,   0.73290604,
          0.72160196,   0.72022319,   0.72175837,   0.7149834 ,
          0.71019107,   0.69324499,   0.67326212,   0.66642427,
          0.6603598 ,   0.65269005,   0.66781014,   0.66870576,
          0.68053657,   0.68605399,   0.70080197,   0.69739068,
          0.68165374,   0.69695055,   0.71426642,   0.71511525,
          0.713682  ,   0.68909109,   0.67233878,   0.65546823,
          0.65481389,   0.64934373,   0.63906163,   0.57418382,
          0.54665542,   0.54188067,   0.53819233,   0.60591334,
          0.63904619,   0.66168606,   0.68078768,   0.67073548,
          0.66508675,   0.66622818,   0.65382832,   0.66477919,
          0.6809727 ,   0.69796801,   0.71139133,   0.69840521,
          0.72514206,   0.72456962,   0.71542597,   0.7171694 ,
          0.74917126,   0.75830919,   0.73841745,   0.72931463,
          0.72458988,   0.71088988,   0.71179658,   0.73096341,
          0.7112304 ,   0.70124453,   0.60252255,   0.5600487 ,
          0.54079241,   0.62936461,   0.67191106,   0.68853861,
          0.7077437 ,   0.71209711,   0.70631182,   0.70821148,
          0.70690024,   0.71529031,   0.71166706,   0.71813565,
          0.70805389,   0.69250953,   0.68530262,   0.70589399,
          0.71437168,   0.71995121,   0.71334577,   0.73829931,
          0.73541772,   0.73075122,   0.72565365,   0.75073779,
          0.75378919,   0.71968502,   0.71273291,   0.69417143,
          0.67019778,   0.65785867,   0.66234952,   0.64876282,
          0.64418197,   0.58545005,   0.55859298,   0.54224533,
          0.60941398,   0.63490164,   0.64758009,   0.65305245,
          0.64515799,   0.63654613,   0.6284008 ,   0.62690061,
          0.65086871,   0.66105002,   0.67813903,   0.67918772,
          0.68308616,   0.68892872,   0.68981707,   0.69851744,
          0.71327049,   0.70474863,   0.69256419,   0.67079401,
          0.65518093,   0.64557105,   0.64220029,   0.63617468,
          0.79227799,  -3.51146007,  -0.8932817 ,  -0.8292731 ,
         -3.24586916,  -0.12381294,   0.25702363,  -0.59987533,
         -0.81159002,  -0.2411634 ,  -4.75630379,  -0.11473341,
         -0.31742597,  -0.95632511,  -0.44578609,  -0.71053219,
         -7.69628143,  -0.10928281,  -0.31042406,  -0.92769933,
         -0.4685356 ,  -0.58194721,  -7.43715429,  -0.15127681,
          0.41375017,  -0.88155276,  -1.3530277 ,  -0.57746065,
         -7.42206144,  -0.08162763,   0.16980466,  -0.61480427,
         -0.66983718,  -3.44586134,  -3.65565228,  -0.0823019 ,
          0.18377568,  -0.61387658,  -0.68719655,  -3.10081434,
         -3.45911002,  -0.35725731,  -0.88959008,  -1.7191509 ,
         -1.03030622,  -0.917974  ,  -6.3901329 ,  -0.34188899,
         -0.87650335,  -1.69257486,  -1.04027009,  -0.85638893,
         -6.59318495,  -0.05210859,  -0.15308192,  -0.87337536,
         -0.29142395,  -3.27884984,  -4.9951601 ,  -0.05089432,
         -0.13649182,  -0.88233888,  -0.25719821,  -3.51509738,
         -4.96978092,  -0.46915358,  -1.22309649,  -1.87023997,
         -1.16855919,  -0.6669237 ,  -6.29674339,  -0.44613662,
         -1.20296133,  -1.85118794,  -1.0835067 ,  -0.59030527,
         -7.28811884,  -0.41062886,  -1.23562849,  -1.82952428,
         -0.9187994 ,  -0.57888949,  -8.06137276,  -0.37985   ,
         -1.21550822,  -1.81209254,  -0.83225882,  -0.48842043,
         -9.14271736],
       [  1.47111392,   1.49502981,   1.50468314,   1.52963531,
          1.55925441,   1.58597898,   1.61695743,   1.63119209,
          1.65188158,   1.65112901,   1.65239537,   1.66497755,
          1.67322683,   1.6996156 ,   1.74297941,   1.76659966,
          1.78235221,   1.7916342 ,   1.76789761,   1.75706923,
          1.73877347,   1.73180008,   1.71150172,   1.70980012,
          1.72339141,   1.7082001 ,   1.69214332,   1.69332492,
          1.69562507,   1.74324048,   1.77722967,   1.80241346,
          1.80515194,   1.81041539,   1.82009757,   2.0614562 ,
          2.18523335,   2.21469712,   2.21837378,   1.97346497,
          1.8431592 ,   1.77101457,   1.73811662,   1.75299883,
          1.75508308,   1.77254438,   1.79673636,   1.77662408,
          1.73131943,   1.70414519,   1.67941117,   1.69232225,
          1.64917099,   1.64713764,   1.66076028,   1.66266763,
          1.61991417,   1.60454988,   1.62164629,   1.63398266,
          1.6558826 ,   1.68963099,   1.70234072,   1.67186093,
          1.70011127,   1.71530759,   1.99147117,   2.09887075,
          2.1434164 ,   1.88568401,   1.76157343,   1.71544611,
          1.68758202,   1.68046141,   1.68784165,   1.67955029,
          1.68046319,   1.66599953,   1.67032886,   1.6622678 ,
          1.68443203,   1.71358573,   1.72772884,   1.69174027,
          1.68654633,   1.67621136,   1.67743003,   1.63342535,
          1.63790345,   1.65816486,   1.67333329,   1.64282262,
          1.64099455,   1.68106008,   1.69613135,   1.73199034,
          1.76513612,   1.78820479,   1.77725303,   1.80975986,
          1.83134675,   2.07188272,   2.17190433,   2.21517181,
          1.97284269,   1.86208761,   1.80850983,   1.79992962,
          1.82250202,   1.84318352,   1.8567034 ,   1.85546744,
          1.82155788,   1.79560375,   1.76066339,   1.74226725,
          1.72835302,   1.72643256,   1.73427248,   1.72960675,
          1.70763969,   1.71169174,   1.72841263,   1.77419019,
          1.80274236,   1.81299615,   1.82583559,   1.8437531 ,
          1.18387926,   1.66575396,   0.63065016,   1.61849594,
          4.18461275,   0.14270517,   0.51769435,   0.47771817,
          0.72500408,   0.11040128,   3.40354562,   0.13009316,
         -0.18212497,   0.7071119 ,   0.30586734,   0.87731177,
          5.07596493,   0.12714678,  -0.18246856,   0.74448448,
          0.34457716,   0.86146623,   5.24312401,   0.30632678,
          0.84221137,   0.847983  ,   1.13917828,   0.24148114,
          5.60893202,   0.12073349,   0.39001611,   0.48501632,
          0.58363879,   1.49184811,   2.52697754,   0.12754926,
          0.40491089,   0.47194228,   0.59487838,   0.54106617,
          2.31779623,   0.30343184,  -0.49257278,   1.24272788,
          0.71459156,   0.48656762,   5.98726225,   0.3053517 ,
         -0.50473642,   1.37695181,   0.76716191,   0.56802279,
          6.20354986,   0.04702853,  -0.05196139,   0.6758486 ,
          0.25732684,   3.01113939,   3.75101089,   0.05117951,
         -0.03982695,   0.63778108,   0.21364087,   2.281533  ,
          3.59415889,   0.39437586,  -0.67277986,   1.65084136,
          0.96368027,   0.62217236,   8.4931469 ,   0.40397301,
         -0.6953941 ,   1.78102195,   0.91787279,   0.62518257,
          9.01820946,   0.47600806,  -0.70763177,   1.71049118,
          1.00366211,   0.69063246,  12.93024826,   0.47936273,
         -0.72998667,   1.81443799,   0.92729098,   0.67199892,
         13.51691723]],
"comment__": "Action  phase, root forward,  swing hip sagital,                 feedback cd  cv,     coronal,   swing knee z,                    stance hip,         stance knee,     swing ankle x,        stance ankle x,      feedback cd cv,  stance shoulder,        swing shoulder,              stance elbow,                     swing elbow,                    pelvis_torso ",
"action_bounds":   [[ 0.49,  -0.25, -0.25,  -0.75,      -0.75,     -0.75,     -0.75,      -0.75,    -0.3,     -0.1,     -0.1,     -0.1,        -0.75,-0.75, -0.75, -0.3,     -0.3,  -1.5,      -1.5,      -1.5,     -1.5,       0.0,     0.0,   -2.25, -2.25,  -2.25,   -2.25,    -2.25,  -2.25,     -0.25,    -0.25,     -0.25,       -0.25,     -0.25,     -0.25,    -0.7   ],
				     [0.51,   0.25,  0.25,   0.75,       0.75,      0.75,      0.5,       0.5,       0.3,      1.75,     1.75,     1.75,        0.75, 0.75,  0.75,  1.3,      1.3,   2.0,       2.0,       2.0,      2.0,       0.4,     0.4,    2.25,  2.25,   2.25,    2.25,     2.25,   2.25,      4.25,     4.25,      4.25,        4.25,      4.25,      4.25,     0.7   ]],
"discrete_actions": [[0.5,   0.06, 0.03,    -0.5500000, -0.366476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,   0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.6,  -0.16, 0.074,   -0.4500000, -0.266476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,   0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.55,  0.06, 0.14,    -0.500000,  -0.216476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,   0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
	             	 [0.45, -0.8, -0.045,   -0.500000,  -0.316476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.65, -0.08, 0.155,   -0.500000,  -0.266476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
	             	 [0.55,  0.08, 0.045,   -0.4500000, -0.266476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.5,  -0.1,  0.05,    -0.500000,  -0.266476, -0.103272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
         			 [0.45,  0.1, -0.165,   -0.400000,  -0.266476, -0.083272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0],
		             [0.61,  0.1,  0.2,     -0.500000,  -0.266476, -0.093272, -0.300000, -0.300000, -0.084616, 0.292735, 1.180385, 0.080433,	0.0,  0.0,   0.0,  	0.0, 	  0.0,   1.080501, -0.034284, -0.141332, 0.379757,  0.15,    0.2,   -0.1,   0.0,    0.1,     0.043195, 0.0,   -0.052132,  0.218452, 0.000000, -0.153725,    0.043875,  0.000000, -0.037823, 0.0]],	  
"action_space_continuous":true,
"train_on_validation_set":true,
"environment_type": "simbiconBiped2DTerrain",
"forward_dynamics_predictor": "network",
"sampling_method": "SequentialMC",
"use_actor_policy_action_suggestion": true,
"num_uniform_action_samples": 3,
"look_ahead_planning_steps": 2,
"plotting_update_freq_num_rounds": 10,
"saving_update_freq_num_rounds": 10,
"num_available_threads": 1,
"queue_size_limit": 1000,
"sim_action_per_training_update": 8,
"adaptive_samples": 5,
"num_adaptive_samples_to_keep": 50,
"use_actor_policy_action_variance_suggestion": false,
"exploration_method": "gaussian_random",
"dropout_p": 0.1,
"regularization_weight": 0.00001,
"rho": 0.95,
"rms_epsilon": 0.000001,
"steps_until_target_network_update": 1000,
"epsilon_annealing": 0.8,
"state_normalization": "given",
    "comment__": "load a pretrained model for the controller",
"load_saved_model": false,
	"comment__": "Number of updates the critic should perform per actor update",
"critic_updates_per_actor_update": 10,
"clamp_actions_to_stay_inside_bounds": true,
"bootstrap_samples": 100,
"bootsrap_with_discrete_policy": true,
"max_epoch_length": 2,
"reward_lower_bound": -0.5,
"use_guided_policy_search" : false,
"training_updates_per_sim_action": 1,
"use_sampling_exploration": false,
"use_model_based_action_optimization": true,
"use_transfer_task_network": false,
"penalize_actions_outside_bounds": false,
"use_transfer_task_network": false,
"forward_dynamics_model_type": "model.ForwardDynamicsDenseNetworkDropout.ForwardDynamicsDenseNetworkDropout",
"save_experience_memory": false,
"train_rl_learning": true,
"use_back_on_track_forcing": false,
"visualize_forward_dynamics": false,
"fd_learning_rate": 0.01,
"train_actor": true,
"debug_critic": true,
"critic_regularization_weight": 0.000001,
"critic_learning_rate": 0.01,
"visualize_expected_value": true,
"target_velocity_decay":-2.0,
"target_velocity":1.0,
"num_terrain_features": 128,
"initial_temperature": 5.0,
"min_epsilon": 0.15,
"shouldRender": false,
    "comment__": "Learning rate use for model based action exploration",
"action_learning_rate": 10.0,
    "comment__": "During model-based action exloration, Probability of a random action being generated from MBAE,",
"model_based_action_omega": 0.0,
	"comment__": "visualize the loss and regularization cost of the actor. Warning: might use a lot of memory",
"debug_actor": true,
	"comment__": "float type to use, if for example you want to train on the GPU use float32",
"float_type": "float64",
	"comment__": "What processor type to perform the training on [cpu|gpu]",
"training_processor_type": "cpu",
	"comment__": "optimizer is the type of optimization algorithm to use",
"optimizer": "adam",
	"comment__": "This setting uses a sampling method overtop of simulation",
"use_simulation_sampling": false,
	"comment__": "Variance scaling used for sampling",
"variance_scalling": 0.1,
	"comment__": "Whether or not to parameterize the control via the reward function, also adds parameters to input state",
"use_parameterized_control": false,
"controller_parameter_settings": {
		"velocity_bounds": [[0.5],[2.5]],
		"root_height_bounds": [[0.7],[1.1]],
		"root_pitch_bounds": [[-0.25],[0.25]],
		"right_hand_x_pos_bounds": [[0.0],[0.5]],
		"torque_bounds": [[0.0],[0.5]]
		},
	"comment__": "THe parameter used to control the average change in the control parameters",
"average_parameter_change": 0.2,
	"comment__": "Whether or not to train the value function some output from the forward dynamics",
"train_critic_on_fd_output": false,
	"comment__": "Use to add an additional regularization term to prevent the network from moving to far from its previous values",
"use_previous_value_regularization": false,
	"comment__": "Controls the level of information that is printed to the terminal",
"print_level": "train",
	"comment__": "print level descriptions",
"print_levels": {
		"debug": 1,
		"train": 0,
		"hyper_train": -1
		},
	"comment__": "The type of function to apply over the controller target values [gaussian|abs]",
"reward_smoother": "gaussian",
	"comment__": "Weights for different components of the reward function",
"controller_reward_weights": {
		"velocity": 0.8,
		"torque": 0.05,
		"root_height": 0.05,
		"root_pitch": 0.1,
		"right_hand_x_pos": 0.0
		},
	"comment__": "Regularization weight for different between policy parameters and old policy parameters",
"previous_value_regularization_weight":  0.001,
	"comment__": "Random seed value for the simulation to use",
"random_seed": 1234,
	"comment__": "Makes a few changes to the flow of control in order for things to be on policy",
"on_policy": true,
	"comment__": "Whether or not to use a stochastic policy, This adds more outputs to the network and changes the way actions are sampled",
"use_stocastic_policy": false,
	"comment__": "Whether or  not to train the critic at all. Usually used for debugging",
"train_critic": true,
	"comment__": "Whether or not to collects tuples in batches, this can be good for multi-threading or computing furture discounted reward",
"collect_tuples_in_batches":true,
	"comment__": "Whether or not the controller should be reset to a new epoch when a fall (fallen into some kind of non-recoverable state) has occured",
"reset_on_fall":true
}
