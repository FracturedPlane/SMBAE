{
"model_type": "Deep_NN",
"agent_name": "A3C",
"data_folder": "Simple_Walk2_Parameterized/",
"epsilon": 0.5, 
"omega": 0.15,
"batch_size": 32,
"learning_rate": 0.001,
"sim_config_file": "../data/simbicon3D/trainContinuousActions.ini",
"forwardDynamics_config_file": "../data/simbicon3D/trainContinuousActions.ini",
"num_actions": 9,
"exploration_rate": 0.1,
"rounds": 5000,
"epochs": 10,
"eval_epochs": 10,
"discount_factor": 0.8,
"visualize_learning": true,
"save_trainData": true,
"train_forward_dynamics": false,
"reward_bounds": [[0.0],[1.0]],
"expereince_length": 10000,
"state_bounds": [[ -6.34096220e-01,   9.42412945e-01,  -2.77025047e+00,
          5.99883344e-01,  -5.19683588e-01,  -5.19683588e-01,
         -3.42888109e-01,  -5.23556180e-01,   7.76794216e-02,
         -9.48665544e-02,  -1.25781827e+00,  -5.54537258e-01,
         -1.47478797e-02,   3.06775891e-01,  -5.49878858e-02,
         -3.10999480e-01,  -5.23225430e-01,   4.41216961e-02,
          6.70018120e-02,  -2.68502903e-01,  -1.07900884e-01,
         -3.31729491e-01,  -8.55080422e-01,  -4.32993609e-01,
         -1.05998050e-01,  -2.68106419e-01,  -1.07713842e-01,
         -2.50738648e-01,  -8.67029229e-01,  -4.42178656e-01,
         -2.98209530e-02,   6.47656790e-01,  -6.87410339e-02,
         -3.04862383e-01,  -5.17820447e-01,  -2.19130177e-02,
          1.87506175e-01,   2.88989499e-01,  -5.06674074e-02,
         -3.06572773e-01,  -5.28246094e-01,   9.33682034e-03,
         -2.12056797e-01,   2.88929341e-01,  -4.98415256e-02,
         -3.06224259e-01,  -5.28127163e-01,   1.84725992e-03,
         -5.74297924e-03,  -7.39800415e-01,  -3.28428126e-01,
         -2.37915139e-01,  -1.49736493e+00,  -8.38406722e-01,
         -1.22256339e-01,  -7.39480049e-01,  -3.29799656e-01,
         -2.03172964e-01,  -1.51351519e+00,  -8.77449749e-01,
          1.85604062e-01,  -1.00539563e-01,  -6.78502488e-02,
         -3.08624715e-01,  -5.32719181e-01,  -2.79023435e-01,
         -2.13983417e-01,  -1.00673412e-01,  -6.55732138e-02,
         -3.12425854e-01,  -5.33302855e-01,  -3.10067463e-01,
         -4.53427219e-02,  -1.00490585e+00,  -3.94977763e-01,
         -2.29839893e-01,  -1.30519213e+00,  -7.17686616e-01,
         -1.29773048e-01,  -1.00561552e+00,  -3.99412635e-01,
         -3.34232676e-01,  -1.31537288e+00,  -7.46435832e-01,
         -4.72069519e-02,  -1.02928355e+00,  -3.19346926e-01,
         -1.22069807e-01,  -4.60775992e-01,  -7.68291495e-01,
         -1.24429322e-01,  -1.02959452e+00,  -3.23858854e-01,
         -1.56321658e-01,  -4.63447258e-01,  -7.87323291e-01],
       [  5.52303145e-01,   1.04555761e+00,   3.02666000e+01,
          1.00002054e+00,   8.06735146e-01,   8.06735146e-01,
          3.51870125e-01,   9.91835992e-02,   1.46733007e+00,
          1.20645504e-01,   1.27079125e+00,   5.69349748e-01,
          1.47486517e-02,   4.01719900e-01,   3.42407443e-03,
          3.17251037e-01,   1.04200792e-01,   1.46485770e+00,
          1.07099820e-01,  -2.24500388e-01,   1.59571481e-01,
          2.57378948e-01,   1.38080353e-01,   1.54077880e+00,
         -6.78278735e-02,  -2.24941047e-01,   1.59509411e-01,
          3.43361385e-01,   1.41715459e-01,   1.53683745e+00,
          2.97725769e-02,   6.67872076e-01,   7.29232433e-02,
          3.09939696e-01,   1.23269819e-01,   1.36720212e+00,
          2.12060324e-01,   3.09593576e-01,   2.78002296e-02,
          3.12247037e-01,   1.11951187e-01,   1.48659232e+00,
         -1.87558614e-01,   3.09643560e-01,   2.65821725e-02,
          3.12895954e-01,   1.12183498e-01,   1.49365569e+00,
          1.25905392e-01,  -6.39025192e-01,   3.38600265e-01,
          1.98073892e-01,   4.55785665e-01,   1.35369655e+00,
          2.88702715e-03,  -6.39567527e-01,   3.43208581e-01,
          2.41450683e-01,   4.51482942e-01,   1.37950851e+00,
          2.13967233e-01,  -8.02279111e-02,   3.96722428e-02,
          3.18095279e-01,   1.06835040e-01,   1.63583511e+00,
         -1.85688622e-01,  -8.01561112e-02,   3.65994168e-02,
          3.14904357e-01,   1.08489792e-01,   1.66426497e+00,
          1.34669408e-01,  -9.01365767e-01,   3.74676050e-01,
          3.23646209e-01,   5.41831850e-01,   1.34261187e+00,
          4.15927916e-02,  -9.00870526e-01,   3.85834112e-01,
          2.30340241e-01,   5.35241615e-01,   1.38430484e+00,
          1.29370966e-01,  -9.27492425e-01,   4.43455684e-01,
          1.60730840e-01,   2.58810565e-01,   1.61350876e+00,
          4.36603207e-02,  -9.27366355e-01,   4.54681722e-01,
          1.24063643e-01,   2.61238771e-01,   1.64705196e+00]],
"comment__": "Action  phase, root forward,  swing hip sagital,                 coronal,  swing knee z,               ",
"action_bounds":   [[ 0.49,  -0.25, -0.25,     -0.6,       -0.6,      -0.6,      -0.3,     -0.1,     -0.1,     -0.1     ],
				     [0.51,   0.25,  0.25,      0.6,        0.6,       0.6,       0.3,      1.5,      1.5,      1.5    ]],
"discrete_actions": [[0.5,   0.06, 0.03,    -0.5500000, -0.366476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.6,  -0.16, 0.074,   -0.4500000, -0.266476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.55,  0.06, 0.14,    -0.500000,  -0.216476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.45, -0.8, -0.045,   -0.500000,  -0.316476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.65, -0.08, 0.155,   -0.500000,  -0.266476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.55,  0.08, 0.045,   -0.4500000, -0.266476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.5,  -0.1,  0.05,    -0.500000,  -0.266476, -0.103272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.45,  0.1, -0.165,   -0.400000,  -0.266476, -0.083272, -0.084616, 0.292735, 1.180385, 0.080433],
		             [0.61,  0.1,  0.2,     -0.500000,  -0.266476, -0.093272, -0.084616, 0.292735, 1.180385, 0.080433]],	  
"action_space_continuous":true,
"train_on_validation_set":true,
"environment_type": "simbiconBiped3D",
"forward_dynamics_predictor": "network",
"sampling_method": "SequentialMC",
"use_actor_policy_action_suggestion": true,
"num_uniform_action_samples": 3,
"look_ahead_planning_steps": 2,
"plotting_update_freq_num_rounds": 10,
"saving_update_freq_num_rounds": 10,
"num_available_threads": 12,
"queue_size_limit": 100,
"sim_action_per_training_update": 8,
"adaptive_samples": 5,
"num_adaptive_samples_to_keep": 50,
"use_actor_policy_action_variance_suggestion": false,
"exploration_method": "gaussian_random",
"dropout_p": 0.1,
"regularization_weight": 0.00001,
"rho": 0.95,
"rms_epsilon": 0.000001,
"steps_until_target_network_update": 1000,
"epsilon_annealing": 0.8,
"state_normalization": "variance",
    "comment__": "load a pretrained model for the controller",
"load_saved_model": false,
	"comment__": "Number of updates the critic should perform per actor update",
"critic_updates_per_actor_update": 1,
"clamp_actions_to_stay_inside_bounds": true,
"bootstrap_samples": 1000,
"bootsrap_with_discrete_policy": true,
"max_epoch_length": 100,
"reward_lower_bound": -0.5,
"use_guided_policy_search" : false,
"training_updates_per_sim_action": 1,
"use_sampling_exploration": false,
"use_model_based_action_optimization": true,
"use_transfer_task_network": false,
"penalize_actions_outside_bounds": false,
"use_transfer_task_network": false,
"forward_dynamics_model_type": "model.ForwardDynamicsDenseNetworkDropout.ForwardDynamicsDenseNetworkDropout",
"save_experience_memory": false,
"train_rl_learning": true,
"use_back_on_track_forcing": false,
"visualize_forward_dynamics": false,
"fd_learning_rate": 0.01,
"train_actor": true,
"debug_critic": true,
"critic_regularization_weight": 0.000001,
"critic_learning_rate": 0.01,
"visualize_expected_value": true,
"target_velocity_decay":-2.0,
"target_velocity":1.0,
"num_terrain_features": 0,
"initial_temperature": 5.0,
"min_epsilon": 0.15,
"shouldRender": false,
    "comment__": "Learning rate use for model based action exploration",
"action_learning_rate": 10.0,
    "comment__": "During model-based action exloration, Probability of a random action being generated from MBAE,",
"model_based_action_omega": 0.0,
	"comment__": "visualize the loss and regularization cost of the actor. Warning: might use a lot of memory",
"debug_actor": true,
	"comment__": "float type to use, if for example you want to train on the GPU use float32",
"float_type": "float64",
	"comment__": "What processor type to perform the training on [cpu|gpu]",
"training_processor_type": "cpu",
	"comment__": "optimizer is the type of optimization algorithm to use",
"optimizer": "adam",
	"comment__": "This setting uses a sampling method overtop of simulation",
"use_simulation_sampling": false,
	"comment__": "Variance scaling used for sampling",
"variance_scalling": 0.1,
	"comment__": "Whether or not to parameterize the control via the reward function, also adds parameters to input state",
"use_parameterized_control": true,
	"comment__": "parameter bounds for parameterized controller",
"controller_parameter_settings": {
		"velocity_bounds": [[0.5],[2.5]],
		"root_height_bounds": [[0.7],[1.1]]
		}
}
