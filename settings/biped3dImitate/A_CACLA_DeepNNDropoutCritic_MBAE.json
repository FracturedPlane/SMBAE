{
    "comment__": "Type of model/network to use for the actor and critic",
"model_type": "Deep_NN_Dropout_Critic",
    "comment__": "Learning algorithm to use",
"agent_name": "A_CACLA",
    "comment__": "Folder to store the training data in.",
"data_folder": "Simple_Walk3D_Imitate_MBAE/",
	"comment": "initial probability of selecting a random action",
"epsilon": 0.55, 
	"comment": "initial probability of selecting a discrete random action",
"omega": 0.0,
    "comment__": "Batch size used for learning",
"batch_size": 32,
    "comment__": "Learning rate for the actor/policy",
"learning_rate": 0.0001,
    "comment__": "Config file for the simulator",
"sim_config_file": "../data/simbicon3D/trainbiped3DImitation.ini",
    "comment__": "A different config file for the forward dynamics simulator",
"forwardDynamics_config_file": "../data/simbicon3D/trainbiped3DImitation.ini",
    "comment__": "Exploration rate use when randomly generating new actions",
"exploration_rate": 0.15,
    "comment__": "Number of rounds to perform before termination",
"rounds": 500000,
    "comment__": "Number of epochs to perform per round",
"epochs": 10,
    "comment__": "Number of epoch/episode to evaluate the policy over",
"eval_epochs": 10,
    "comment__": "Discount factor used during learning",
"discount_factor": 0.95,
    "comment__": "Should the training be plotted during learning",
"visualize_learning": true,
    "comment__": "Whether or not to save the plotted data while learning",
"save_trainData": true,
    "comment__": "Whether or not to train a forward dynamics model as well",
"train_forward_dynamics": true,
    "comment__": "Bounds used for scaling rewards for networks",
"reward_bounds": [[0.0],[1.0]],
    "comment__": "Max length of the Experience memory",
"expereince_length": 10000,
    "comment__": "Possible state bounds to be used for scaling states for networks",
"state_bounds": [[  1.23165673e-02,  -1.00867889e+00,  -6.58729979e-01,
         -5.68158360e-01,  -8.97857935e-01,  -4.91986838e-01,
         -8.69646895e-01,  -4.96960888e-01,  -4.11189204e+00,
         -2.72529447e+00,  -3.88299197e+00,   4.16400381e-02,
         -1.61591345e-01,  -2.21627502e-01,  -3.94030764e-01,
         -9.70946307e-01,  -5.01210981e-01,   1.07541587e+00,
         -5.74700723e-01,  -2.26811793e-01,  -1.09743094e+00,
         -2.77140618e+00,  -2.58755072e+00,  -3.28209892e+00,
         -2.34539213e-01,  -1.06968083e-01,  -1.19347835e-01,
         -7.51129317e-01,  -8.85800612e-01,  -5.87232817e-01,
         -5.86973044e-01,  -4.15535074e-01,  -1.99291456e-01,
         -9.54383894e-01,  -5.38007024e+00,  -4.66023793e+00,
         -4.79684498e+00,  -2.62467613e-01,  -1.15730466e-01,
         -1.77549899e-01,  -7.23786798e-01,  -9.72955049e-01,
         -6.38854289e-01,   1.09294965e+00,  -5.24575091e-01,
         -2.65294808e-01,  -1.07573926e+00,  -4.20796314e+00,
         -3.69585945e+00,  -4.90070732e+00,  -6.58254881e-01,
         -2.83024070e-01,  -3.81041068e-01,  -1.52576190e+00,
         -1.38702705e+00,  -1.51591559e+00,  -5.87967480e-01,
         -3.96572205e-01,  -1.95192773e-01,  -9.66983326e-01,
         -5.40660577e+00,  -4.72192934e+00,  -4.75808643e+00,
         -6.41112715e-01,  -2.62123015e-01,  -3.88494321e-01,
         -1.50036069e+00,  -1.45013699e+00,  -1.54201706e+00,
          2.28886655e-01,  -4.62539526e-01,  -3.84782857e-01,
         -1.04487851e+00,  -5.06315651e+00,  -6.28426234e+00,
         -5.84360067e+00,  -9.01080393e-01,  -3.98364400e-01,
         -5.36574249e-01,  -2.17692855e+00,  -1.85060946e+00,
         -2.18507716e+00,  -8.27144475e-01,  -4.16306139e-01,
         -2.62801566e-01,  -9.67686314e-01,  -6.10779287e+00,
         -6.00678627e+00,  -5.23840874e+00,  -8.57312605e-01,
         -3.67349302e-01,  -5.22776216e-01,  -2.49357746e+00,
         -2.17249181e+00,  -2.70352544e+00,   2.45943853e-01,
         -4.96393627e-01,  -4.47848120e-01,  -1.02215081e+00,
         -5.35005407e+00,  -6.86695186e+00,  -6.20295154e+00,
          3.35005835e-01,   1.80559726e-01,  -2.55832139e-01,
         -8.45837490e-01,  -9.22646178e-01,   1.15862618e-01,
          1.52231792e-01,  -6.51165784e-01,   7.19364368e-01,
          1.93036370e-01,   1.20901468e-01,  -6.25772834e-01,
         -2.03170043e-02,   2.23127689e-01,   1.02336009e-01,
         -5.83015197e-01,   4.66827262e-02,   5.81649258e-02,
         -0.10000000e+00,  -0.10000000e+00,  -1.10000000e+00,
          8.03159380e-02,  -0.10000000e+00,  -0.10000000e+00,
         -1.10000000e+00,   1.39963148e-01,  -7.61468308e-01,
         -7.25145511e-01,  -7.00816641e-02,   5.06939698e-01,
         -2.37700908e-01,  -5.88985635e-01,  -5.06199948e-01],
       [  5.20359877e-01,   2.87218954e+00,   5.03687775e-01,
          3.97713724e-01,   1.58884994e-01,   7.75605449e-01,
          5.97407785e-01,   6.84559377e-01,   4.17481426e+00,
          2.79272099e+00,   2.88781448e+00,   4.09654044e-01,
          1.45549239e-01,   2.13662940e-01,   8.23797041e-01,
          5.67358599e-01,   5.65252372e-01,   2.08592782e+00,
          6.11375848e-01,   1.75008606e-01,  -9.75314922e-02,
          3.14885373e+00,   2.72372192e+00,   3.34774688e+00,
         -2.99269069e-03,   1.22272946e-01,   1.73174320e-01,
          8.32788528e-01,   6.96743344e-01,   6.83287031e-01,
          2.65776841e+00,   7.56445879e-01,   5.35827777e-01,
          1.32072424e-01,   5.73383699e+00,   4.32474514e+00,
          4.91395298e+00,  -2.92313317e-04,   1.19341391e-01,
          1.37354657e-01,   9.23247702e-01,   8.16739800e-01,
          7.53932932e-01,   2.15070828e+00,   6.31481142e-01,
          3.11631106e-01,  -3.66375641e-02,   4.26989701e+00,
          3.96106718e+00,   4.66070757e+00,   3.20438826e-02,
          3.17590242e-01,   4.78777465e-01,   1.31461617e+00,
          1.37728007e+00,   1.55869891e+00,   2.65927810e+00,
          7.63811504e-01,   5.25972265e-01,   9.75637147e-02,
          5.72335782e+00,   4.32246520e+00,   4.97844147e+00,
         -2.21166598e-02,   2.65204294e-01,   3.15696993e-01,
          1.61198866e+00,   1.43801571e+00,   1.54574008e+00,
          2.72625592e+00,   5.75021745e-01,   4.61102403e-01,
         -5.44316090e-02,   4.94727557e+00,   6.19286395e+00,
          5.55020856e+00,   7.28968983e-02,   4.34012154e-01,
          6.67331222e-01,   1.78416668e+00,   1.94086342e+00,
          2.22167148e+00,   2.70767661e+00,   7.53585467e-01,
          4.87204870e-01,   1.26285747e-01,   6.39685371e+00,
          5.56630403e+00,   5.48491254e+00,  -2.91662249e-03,
          3.66586799e-01,   4.19593496e-01,   2.58884942e+00,
          2.22843530e+00,   2.59939025e+00,   2.74798383e+00,
          5.81947207e-01,   4.11577545e-01,  -1.32103748e-02,
          5.30372476e+00,   6.57647352e+00,   6.04465439e+00,
          1.40216216e+00,   2.67057007e-01,   1.04049354e-01,
         -1.96418768e-01,  -5.64292732e-01,   1.53137918e-01,
          4.45898239e-01,  -7.30764282e-03,   9.14573599e-01,
          5.31552263e-01,   6.32357772e-01,   3.24859067e-01,
          1.04305736e+00,   5.51477267e-01,   5.95753574e-01,
          3.84364058e-01,   1.06994546e+00,   6.65888009e-01,
          0.10000000e+00,   0.10000000e+00,  -0.90000000e+00,
          7.48814254e-01,   0.10000000e+00,   0.10000000e+00,
         -0.90000000e+00,   3.28197302e-01,  -1.77028995e-01,
          2.20305632e-01,   8.99747036e-01,   1.11172063e+00,
          7.81764586e-01,   4.56644468e-01,   7.21578668e-01]],
"comment__": "Action  pelvisTorso                    rHip                        lHip                        rKnee    lKnee      rAnkle                     lAnkle",
"action_bounds":   [[ -0.5, -1.0, -1.0, -1.0,		-0.4, -1.0, -1.0, -1.0, 	-0.4, -1.0, -1.0, -1.0, 	-3.25, 	 -3.25, 	 -1.0, -2.0, -2.0, -2.0,	-1.0, -2.0, -2.0, -2.0],
				     [ 0.5,  1.0,  1.0,  1.0,  		 3.25, 1.0,  1.0,  1.0,   	 3.25, 1.0,  1.0,  1.0,	 	 0.50, 	  0.50,       2.0,  2.0,  2.0,  2.0,     2.0,  2.0,	 2.0,  2.0]],
    "comment__": "Some hand crafted actions the agent can select from. These are becoming deprecated",
"discrete_actions": [[1.0,   0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0,   1.00, 0.00,   0.0, 1.0, 1.0, 0.0, 0.0,   1.0, 1.0,   0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0],
		             [1.0,   0.0, 0.0, 1.0, -1.0, 0.0, 0.0, 1.0,   1.00, 0.00,   0.0, 1.0, 1.0, 0.0, 0.0,   1.0, 1.0,   0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0]],
    "comment__": "Is action space continuous or discrete?",
"action_space_continuous":true,
    "comment__": "Should the method train on the validation set only",
"train_on_validation_set":true,
    "comment__": "Name of the type of simulator to use",
"environment_type": "mocapImitation2D",
    "comment__": "Model type to use for the forward dynamics model",
"forward_dynamics_predictor": "network",
    "comment__": "Method to be used for the forward dynamics model is the model types uses a simulator",
"sampling_method": "SequentialMC",
    "comment__": "Use the action suggested by the policy to start the sampling method.",
"use_actor_policy_action_suggestion": true,
    "comment__": "If selecting from a uniform distribution the number of regularly distant samples to take / action dimension",
"num_uniform_action_samples": 3,
    "comment__": "Number of steps ahead the actions should be sampled",
"look_ahead_planning_steps": 2,
    "comment__": "How often to update the training data and plots wrt # of rounds",
"plotting_update_freq_num_rounds": 10,
    "comment__": "How often to save the training data and plotting data",
"saving_update_freq_num_rounds": 10,
    "comment__": "Number of treads that can be run in parallel during training",
"num_available_threads": 10,
    "comment__": "Length of the queues used to pass simulation data between the simulation workers and the learning agent(s).",
"queue_size_limit": 1000,
    "comment__": "Number of actions performed between training updates",
"sim_action_per_training_update": 16,
    "comment__": "Number of rounds of adaptive sampling",
"adaptive_samples": 5,
    "comment__": "Number of elite adaptive samples to keep between adaptive sampling rounds",
"num_adaptive_samples_to_keep": 50,
    "comment__": "Use the variance calculated from the policy network (calculated using dropout)",
"use_actor_policy_action_variance_suggestion": false,
    "comment__": "Method used for action exploration",
"exploration_method": "gaussian_random",
    "comment__": "Amount of dropout to use in the networks (if using a dropout network)",
"dropout_p": 0.1,
    "comment__": "Regularization weight for the policy network",
"regularization_weight": 0.000001,
    "comment__": "Some parameter for rmsprop stochastic gradient optimization method.",
"rho": 0.95,
    "comment__": "Some parameter for rmsprop stochastic gradient optimization method.",
"rms_epsilon": 0.000001,
    "comment__": "Number of training updates before the target network is updated",
"steps_until_target_network_update": 500,
    "comment__": "Initial factor epsilon in multiplied by (This value will slowly be reduced during training)",
"epsilon_annealing": 0.8,
    "comment__": "Different ways of calculating the scaling method used normalize the input and outputs of the network from the bootstrapping samples. minmax, input and output are -mean/max-min. variance, input and output are -mean/(std*2), given, use the bounds provided in this file",
"state_normalization": "given",
    "comment__": "load a pretrained model for the controller",
"load_saved_model": false,
	"comment__": "Number of updates the critic should perform per actor update",
"critic_updates_per_actor_update": 2,
    "comment__": "weather or not to clamp actions to stay inside the action boundaries",
"clamp_actions_to_stay_inside_bounds": true,
    "comment__": "Number of initial actions to sample before calculating input/output scaling and starting to train.",
"bootstrap_samples": 2000,
    "comment__": "What method to use to select actions during bootstrapping",
"bootsrap_with_discrete_policy": true,
    "comment__": "That max number of action that can be take before the end of an episode/epoch",
"max_epoch_length": 200,
    "comment__": "If reward is below this bound it will not be put in the Experience Buffer",
"reward_lower_bound": -0.5,
    "comment__": "Enable guided policy search. Uses MCMC sampling ahead in time to select the best action to keep",
"use_guided_policy_search" : false,
    "comment__": "The number of training updates to perform for every action that is simulated",
"training_updates_per_sim_action": 1,
    "comment__": "Use The forward dynamics simulator as a way of sampling suggested actions for exploration",
"use_sampling_exploration": false,
    "comment__": "Use the forward dyanmics model to perform action exploration wrt to V -> fd > delta Action gradients",
"use_model_based_action_optimization": true,
    "comment__": "Flag for policy evaluation to swap in the task network from one model and the character/robot network from another",
"use_transfer_task_network": false,
    "comment__": "Add a large cost to actions that are suggested to be outside the action bounds.",
"penalize_actions_outside_bounds": false,
    "comment__": "Network type to use for the forward dynamics model",
"forward_dynamics_model_type": "model.ForwardDynamicsDenseNetworkDropout.ForwardDynamicsDenseNetworkDropout",
    "comment__": "Whether or not to save the Experience memory after bootstrapping",
"save_experience_memory": false,
    "comment__": "Whether or not to train the policy and critic?",
"train_rl_learning": true,
    "comment__": "Force the character to start each new action in a good state, close to a good solution",
"use_back_on_track_forcing": false,
    "comment__": "draw/render the next state suggested by the forward dynamics model",
"visualize_forward_dynamics": false,
    "comment__": "Learning rate for the forward dynamics model",
"fd_learning_rate": 0.001,
    "comment__": "Whether or not to train the policy. Used for debugging",
"train_actor": true,
    "comment__": "Plot the terms for the critic as well (regularization and td error)",
"debug_critic": true,
    "comment__": "critic regularization weight",
"critic_regularization_weight": 0.000001,
    "comment__": "Critic learning rate",
"critic_learning_rate": 0.001,
    "comment__": "During evaluation plot of value function",
"visualize_expected_value": true,
    "comment__": "exponential decay value for use in reward function",
"target_velocity_decay":-2.0,
    "comment__": "Target velocity for controller",
"target_velocity":1.0,
    "comment__": "NUmmber of terrain features for which convolutinoal filters should be used",
"num_terrain_features": 0,
    "comment__": "Initial tempurature for annealing of e-greedy exploration",
"initial_temperature": 2.5,
    "comment__": "epsilon lower limit",
"min_epsilon": 0.15,
    "comment__": "Whether or not to draw/render the simulation",
"shouldRender": false,
    "comment__": "Learning rate use for model based action exploration",
"action_learning_rate": 0.1,
    "comment__": "During model-based action exloration, Probability of a random action being generated from MBAE,",
"model_based_action_omega": 0.5,
	"comment__": "visualize the loss and regularization cost of the actor. Warning: might use a lot of memory",
"debug_actor": true,
	"comment__": "float type to use, if for example you want to train on the GPU use float32",
"float_type": "float64",
	"comment__": "What processor type to perform the training on [cpu|gpu]",
"training_processor_type": "cpu",
	"comment__": "optimizer is the type of optimization algorithm to use",
"optimizer": "adam",
	"comment__": "This setting uses a sampling method overtop of simulation",
"use_simulation_sampling": false,
	"comment__": "Variance scaling used for sampling",
"variance_scalling": 0.1,
	"comment__": "Whether or not to parameterize the control via the reward function, also adds parameters to input state",
"use_parameterized_control": false,
	"comment__": "parameter bounds for parameterized controller",
"controller_parameter_settings": {
		"velocity_bounds": [[0.0],[1.5]],
		"root_height_bounds": [[0.7],[1.1]],
		"torque_bounds": [[0.0],[1.0]],
		"pose_error_bounds": [[0.0], [0.25]]
		},
	"comment__": "The parameter used to control the average change in the control parameters",
"average_parameter_change": 0.25,
	"comment__": "Whether or not to train the value function some output from the forward dynamics",
"train_critic_on_fd_output": true,
	"comment__": "Use to add an additional regularization term to prevent the network from moving to far from its previous values",
"use_previous_value_regularization": false,
	"comment__": "Controls the level of information that is printed to the terminal [train|debug]",
"print_level": "train",
	"comment__": "print level descriptions",
"print_levels": {
		"debug": 1,
		"train": 0,
		"hyper_train": -1
		},
	"comment__": "The type of function to apply over the controller target values [gaussian|abs]",
"reward_smoother": "gaussian",
	"comment__": "Weights for different components of the reward function",
"controller_reward_weights": {
		"velocity": 0.25,
		"torque": 0.05,
		"root_height": 0.1,
		"pose_error": 0.6
		},
	"comment__": "Regularization weight for different between policy parameters and old policy parameters",
"previous_value_regularization_weight":  0.001,
	"comment__": "Random seed value for the simulation to use",
"random_seed": 1234,
	"comment__": "Makes a few changes to the flow of control in order for things to be on policy",
"on_policy": false,
	"comment__": "Whether or not to use a stochastic policy, This adds more outputs to the network and changes the way actions are sampled",
"use_stocastic_policy": false,
	"comment__": "Whether or  not to train the critic at all. Usually used for debugging",
"train_critic": true,
	"comment__": "What type of regularization to use",
"regularization_type": "kl",
	"comment__": "Whether or not to collects tuples in batches, this can be good for multi-threading or computing furture discounted reward",
"collect_tuples_in_batches":false,
	"comment__": "Whether or not the controller should be reset to a new epoch when a fall (fallen into some kind of non-recoverable state) has occured",
"reset_on_fall": false,
	"comment__": "The path to a file containing a motion the character should imitate",
"motion_file": "../data/mocap/walk/two_steps.json",
	"comment__": "Whether a model of the reward r <- R(s,a) should be trained",
"train_reward_predictor": true,
	"comment__": "How many gradient steps model based actino exploration should take",
"num_mbae_steps": 1,
    "comment__": "Whether or not the actor buffer should be fixed to process batches of a certain size",
"fix_actor_batch_size": true,
	"comment__": "Whether or not to put MBAE on the annelling schedule",
"anneal_mbae": false,
	"comment__": "How many value function updates should be done between Dyna updates",
"dyna_update_lag_steps": 10
}