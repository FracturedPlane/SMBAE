{
    "dont_use_advantage": false,
    "controller_parameter_settings": {
        "velocity_bounds": [
            [
                0.5
            ],
            [
                2.5
            ]
        ]
    },
    "ppo_use_seperate_nets": true,
    "rounds": 1000,
    "reward_smoother": "gaussian",
    "reward_bounds": [
        [
            -10.5
        ],
        [
            10.5
        ]
    ],
    "bootstrap_samples": 10000,
    "previous_value_regularization_weight": 0.01,
    "exploration_rate": 0.2,
    "penalize_actions_outside_bounds": false,
    "action_learning_rate": 0.5,
    "sampling_method": "SequentialMC",
    "reset_on_fall": true,
    "plotting_update_freq_num_rounds": 5,
    "annealing_schedule": "log",
    "clamp_actions_to_stay_inside_bounds": false,
    "use_random_actions_for_MBAE": false,
    "only_use_exp_actions_for_poli_updates": true,
    "critic_network_layer_sizes": [
        64,
        32,
        16
    ],
    "policy_loss_weight": 1.0,
    "dyna_update_lag_steps": 5,
    "forwardDynamics_config_file": "MembraneStack-v0",
    "epsilon_annealing": 0.8,
    "reward_lower_bound": -300.5,
    "action_bounds": [
        [
            -1.2,
            -1.2,
            -1.2,
            -1.2,
            -1.2
        ],
        [
            1.2,
            1.2,
            1.2,
            1.2,
            1.2
        ]
    ],
    "random_seed": 1234,
    "last_policy_layer_activation_type": "tanh",
    "num_adaptive_samples_to_keep": 50,
    "std_entropy_weight": 0.01,
    "state_bounds": [
        [
            -0.71919572,
            -0.75608228,
            0.18379175,
            -0.73262815,
            -0.04577937,
            -0.11062196,
            -0.08795502,
            -0.14144782,
            -0.95575357,
            -1.07821544,
            -1.04635855,
            -0.96216829,
            -0.94990963,
            -0.33517967,
            -0.24173344,
            -0.24250843,
            -0.25552781,
            -0.32546727
        ],
        [
            -0.10461047,
            -0.43002882,
            0.80484765,
            -0.25923212,
            0.10000692,
            0.1271541,
            0.03407598,
            0.18321985,
            0.379588,
            -0.05236303,
            0.22224642,
            0.38790465,
            0.41594393,
            0.35147699,
            0.23320772,
            0.24250042,
            0.26625592,
            0.33235925
        ]
    ],
    "action_space_continuous": true,
    "learning_rate": 0.0001,
    "float_type": "float64",
    "use_transfer_task_network": false,
    "agent_name": "algorithm.A_CACLA.A_CACLA",
    "debug_actor": false,
    "shouldRender": false,
    "expereince_length": 10000,
    "policy_network_layer_sizes": [
        64,
        32
    ],
    "critic_regularization_weight": 1e-05,
    "save_trainData": true,
    "policy_activation_type": "tanh",
    "kl_divergence_threshold": 0.01,
    "exploration_method": "gaussian_random",
    "use_actor_policy_action_variance_suggestion": false,
    "train_critic_with_fd_data": false,
    "target_velocity_decay": -2.0,
    "train_on_validation_set": false,
    "train_forward_dynamics": true,
    "use_GAE": true,
    "eval_epochs": 32,
    "train_gan_with_gaussian_noise": true,
    "model_based_action_omega": 0.25,
    "sim_action_per_training_update": 8,
    "discrete_actions": [
        [
            -0.92,
            0.8,
            -0.92,
            0.8,
            -0.92,
            0.8,
            -0.92,
            0.8,
            -0.92,
            0.8
        ],
        [
            0.02,
            0.0,
            0.02,
            0.0,
            0.02,
            0.0,
            0.02,
            0.0,
            0.02,
            0.0
        ],
        [
            -0.2,
            0.2,
            -0.2,
            0.2,
            -0.2,
            0.2,
            -0.2,
            0.2,
            -0.2,
            0.2
        ]
    ],
    "additional_on-poli_trianing_updates": 1,
    "normalize_advantage": true,
    "load_saved_fd_model": false,
    "training_processor_type": "cpu",
    "load_saved_model": false,
    "_last_std_policy_layer_activation_type": "sigmoid",
    "override_sim_env_id": false,
    "adaptive_samples": 5,
    "fd_learning_rate": 0.001,
    "max_epoch_length": 256,
    "num_terrain_features": 0,
    "discount_factor": 0.975,
    "disable_parameter_scaling": false,
    "model_type": "model.DeepNNAdaptive.DeepNNAdaptive",
    "sim_config_file": "MembraneStack-v0",
    "dropout_p": 0.1,
    "comment": "initial probability of selecting a discrete random action",
    "clear_exp_mem_on_poli": false,
    "train_critic": true,
    "num_uniform_action_samples": 3,
    "omega": 0.0,
    "randomize_MBAE_action_length": true,
    "visualize_forward_dynamics": false,
    "on_policy": false,
    "initial_temperature": 3.25,
    "anneal_mbae": true,
    "min_epsilon": 0.2,
    "use_sampling_exploration": false,
    "give_mbae_actions_to_critic": true,
    "collect_tuples_in_batches": false,
    "epsilon": 1.0,
    "use_stocastic_policy": false,
    "use_model_based_action_optimization": true,
    "average_parameter_change": 0.25,
    "use_simulation_sampling": false,
    "save_experience_memory": false,
    "visualize_learning": true,
    "print_level": "train",
    "bootsrap_with_discrete_policy": true,
    "use_multiple_policy_updates": true,
    "saving_update_freq_num_rounds": 10,
    "train_gan": true,
    "use_guided_policy_search": false,
    "queue_size_limit": 100,
    "num_mbae_steps": 1,
    "keep_running_mean_std_for_scaling": true,
    "train_rl_learning": true,
    "variance_scalling": 0.1,
    "use_fixed_std": false,
    "print_levels": {
        "hyper_train": -1,
        "train": 0,
        "debug": 1
    },
    "controller_reward_weights": {
        "root_pitch": 0.1,
        "root_height": 0.05,
        "torque": 0.05,
        "velocity": 0.8,
        "right_hand_x_pos": 0.0
    },
    "train_state_encoding": false,
    "training_updates_per_sim_action": 1,
    "critic_updates_per_actor_update": 1,
    "value_function_batch_size": 32,
    "keep_seperate_fd_exp_buffer": false,
    "data_folder": "MembraneStack-v0/SMBAE_GAN/_train_forward_dynamics_True/",
    "environment_type": "open_AI_Gym",
    "train_actor": true,
    "train_reward_predictor": false,
    "use_parameterized_control": false,
    "epochs": 5,
    "fix_actor_batch_size": true,
    "train_critic_on_fd_output": true,
    "activation_type": "tanh",
    "rho": 0.95,
    "forward_dynamics_model_type": "model.ForwardDynamicsDenseNetworkDropoutTesting.ForwardDynamicsDenseNetworkDropoutTesting",
    "num_available_threads": 2,
    "use_std_avg_as_mbae_learning_rate": true,
    "anneal_on_policy": false,
    "forward_dynamics_predictor": "network",
    "GAE_lambda": 0.95,
    "optimizer": "adam",
    "use_previous_value_regularization": false,
    "rms_epsilon": 1e-06,
    "look_ahead_planning_steps": 2,
    "use_actor_policy_action_suggestion": true,
    "num_on_policy_rollouts": 5,
    "regularization_weight": 1e-05,
    "steps_until_target_network_update": 200,
    "visualize_expected_value": true,
    "regularization_type": "kl",
    "target_velocity": 0.0,
    "critic_learning_rate": 0.0001,
    "comment__": "Use average of policy std as mbae learning rate",
    "batch_size": 32,
    "use_back_on_track_forcing": false,
    "debug_critic": false,
    "state_normalization": "variance"
}