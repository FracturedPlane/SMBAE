{
    "comment__": "Type of model/network to use for the actor and critic",
"model_type": "Deep_CNN_Dropout_Critic",
    "comment__": "Learning algorithm to use",
"agent_name": "A_CACLA",
    "comment__": "Folder to store the training data in.",
"data_folder": "Jog_Terrain_Gaps_Test/",
	"comment": "initial probability of selecting a random action",
"epsilon": 1.00, 
	"comment": "initial probability of selecting a discrete random action",
"omega": 0.0,
    "comment__": "Batch size used for learning",
"batch_size": 32,
    "comment__": "Learning rate for the actor/policy",
"learning_rate": 0.0001,
    "comment__": "Config file for the simulator",
"sim_config_file": "./args/biped2D/test_biped_args.txt",
    "comment__": "A different config file for the forward dynamics simulator",
"forwardDynamics_config_file": "./args/biped2D/test_biped_args.txt",
    "comment__": "Exploration rate use when randomly generating new actions",
"exploration_rate": 0.15,
    "comment__": "Number of rounds to perform before termination",
"rounds": 50000,
    "comment__": "Number of epochs to perform per round",
"epochs": 10,
    "comment__": "Number of epoch/episode to evaluate the policy over",
"eval_epochs": 10,
    "comment__": "Discount factor used during learning",
"discount_factor": 0.9,
    "comment__": "Should the training be plotted during learning",
"visualize_learning": true,
    "comment__": "Whether or not to save the plotted data while learning",
"save_trainData": true,
    "comment__": "Whether or not to train a forward dynamics model as well",
"train_forward_dynamics": false,
    "comment__": "Bounds used for scaling rewards for networks",
"reward_bounds": [[0.0],[1.0]],
    "comment__": "Max length of the Experience memory",
"expereince_length": 10000,
    "comment__": "Possible state bounds to be used for scaling states for networks",
"state_bounds": [[  -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	         -8.24286021e-02,  -8.38266033e-02,  -8.38266033e-02,
	         -8.38266033e-02,  -8.38266033e-02,  -8.38266033e-02,
	         -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	         -9.28703797e-02,  -1.45612763e-01,  -1.84585993e-01,
	         -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	         -2.46396721e-01,  -2.93209221e-01,  -4.63175232e-01,
	         -7.98916273e-01,  -1.26365652e+00,  -1.69129479e+00,
	         -2.03138520e+00,  -2.36060197e+00,  -2.56931018e+00,
	         -2.70748252e+00,  -2.79171297e+00,  -2.67669935e+00,
	         -2.19266193e+00,  -1.21081031e+00,  -3.19811043e-01,
	         -8.70281301e-02,  -4.62199483e-02,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	          -2.14350019e-01,  -2.22621931e-01,  -2.28666506e-01,
	         -3.39107804e-02,  -1.00056621e-01,  -1.48221298e-01,
	         -1.98365192e-01,  -2.24458337e-01,  -2.28981003e-01,
	         -2.23666641e-01,  -2.29340680e-01,  -2.52902928e-01,
	         -2.59656549e-01,  -2.54786254e-01,  -2.83251044e-01,
	         -2.79158747e-01,  -2.79091657e-01,  -2.91002636e-01,
	         -2.77804616e-01,  -2.70602805e-01,  -2.77115344e-01,
	         -2.88332041e-01,  -3.03294560e-01,  -3.36494881e-01,
	         -3.26984573e-01,  -3.25040721e-01,  -3.26374052e-01,
	         -3.45222235e-01,  -3.34384649e-01,  -3.15429865e-01,
	         -3.10816822e-01,  -3.06534688e-01,  -3.15163187e-01,
	         -3.17618431e-01,  -3.18659353e-01,  -3.01862214e-01,
	         -3.11611590e-01,  -3.13349903e-01,  -3.23037469e-01,
	         -3.19736537e-01,  -3.02106628e-01,  -2.88504381e-01,
	         -2.88296995e-01,  -2.65642793e-01,  -2.57265480e-01,
	         -2.69271075e-01,  -3.14653576e-01,  -3.28829953e-01,
	         -3.70189880e-01,  -3.94268600e-01,  -4.09537847e-01,
	         -4.16601816e-01,  -4.24793517e-01,  -4.28421354e-01,
	         -3.92945693e-01,  -4.15060643e-01,  -4.28002593e-01,
	         -4.40559945e-01,  -4.00010117e-01,  -3.58787076e-01,
	         -3.19824967e-01,  -3.24337413e-01,  -3.44106172e-01,
	         -3.22312593e-01,  -3.17250879e-01,  -3.55424638e-01,
	         -4.50395612e-01,  -5.82920480e-01,  -6.95569314e-01,
	         -7.70990627e-01,  -8.34194956e-01,  -8.45252821e-01,
	         -8.33452726e-01,  -8.34718119e-01,  -8.68342192e-01,
	         -9.12985312e-01,  -9.20144395e-01,  -9.17459244e-01,
	         -9.05422561e-01,  -9.30688950e-01,  -9.60004837e-01,
	         -9.36780479e-01,  -9.04301511e-01,  -9.10118645e-01,
	         -9.22669230e-01,  -9.38962876e-01,  -9.82793448e-01,
	         -9.76948700e-01,  -9.70179335e-01,  -9.76414120e-01,
	         -9.82783963e-01,  -9.51212302e-01,  -9.15488760e-01,
	         -8.61813811e-01,  -8.22421284e-01,  -8.37670517e-01,
	         -8.89812344e-01,  -9.04479492e-01,  -8.89188465e-01,
	         -8.68050338e-01,  -8.82998878e-01,  -8.90928829e-01,
	         -8.83186040e-01,  -8.77913222e-01,  -8.76057199e-01,
	         -8.71960453e-01,  -8.66627575e-01,  -8.43119627e-01,
	         -8.27507182e-01,  -8.13013066e-01,  -8.01659577e-01,
	         -7.85800120e-01,  -7.89320035e-01,  -8.16773878e-01,
	         -8.36653990e-01,  -8.80646164e-01,  -9.05535183e-01,
	         -9.42758165e-01,  -9.45282238e-01,   3.68296808e-01,
	         -1.33081437e-01,  -2.53395919e-01,  -2.90042601e-01,
	         -7.16401032e-01,  -2.92123147e-01,  -9.75886109e-01,
	         -7.37076476e-02,  -2.71199170e-01,  -2.45181573e-01,
	         -7.19059987e-01,  -5.24290472e-01,  -1.03933259e+00,
	          9.82980773e-01,  -2.11995225e+00,   1.31402836e+00,
	         -1.29278272e+00,   9.98532936e-01,  -1.17440894e+00,
	         -1.30321573e-01,  -1.62144230e+00,   1.17176123e+00,
	         -1.36007855e+00,   1.01341766e+00,  -2.22728848e+00,
	          9.86232039e-01,  -3.62838577e+00],
	       [  2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          8.07960292e-02,   8.21663417e-02,   8.21663417e-02,
	          8.21663417e-02,   8.21663417e-02,   8.21663417e-02,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          9.00557922e-02,   1.40001015e-01,   1.76288291e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.32299946e-01,   2.69821085e-01,   4.00747375e-01,
	          6.29554188e-01,   9.03400429e-01,   1.11752677e+00,
	          1.21852425e+00,   1.30745611e+00,   1.34248397e+00,
	          1.33895990e+00,   1.36572569e+00,   1.33899829e+00,
	          1.17839965e+00,   7.56722906e-01,   2.48436500e-01,
	          8.49463864e-02,   4.53045205e-02,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          2.04116525e-01,   2.11944854e-01,   2.16529486e-01,
	          3.29771065e-02,   9.68686541e-02,   1.42182849e-01,
	          1.89016714e-01,   2.13730208e-01,   2.17397262e-01,
	          2.11489696e-01,   2.16178474e-01,   2.37034770e-01,
	          2.43732304e-01,   2.38732963e-01,   2.65831093e-01,
	          2.61604857e-01,   2.61141124e-01,   2.71683433e-01,
	          2.59293063e-01,   2.53716171e-01,   2.59141403e-01,
	          2.68695954e-01,   2.80937828e-01,   3.11668244e-01,
	          3.03134222e-01,   3.00499162e-01,   3.01040913e-01,
	          3.20096315e-01,   3.09753493e-01,   2.93087997e-01,
	          2.89378204e-01,   2.85155430e-01,   2.92649005e-01,
	          2.93828843e-01,   2.95347781e-01,   2.79963722e-01,
	          2.90730289e-01,   2.91310427e-01,   2.99864581e-01,
	          2.96994800e-01,   2.80959153e-01,   2.68993778e-01,
	          2.70014235e-01,   2.49652910e-01,   2.42308991e-01,
	          2.51674095e-01,   2.93150151e-01,   3.03871489e-01,
	          3.39513187e-01,   3.58763227e-01,   3.72971118e-01,
	          3.78884856e-01,   3.86574929e-01,   3.90188609e-01,
	          3.54770058e-01,   3.77316843e-01,   3.88478069e-01,
	          4.00651636e-01,   3.64924240e-01,   3.30500957e-01,
	          2.96136985e-01,   3.00898239e-01,   3.19395386e-01,
	          2.98441528e-01,   2.91458868e-01,   3.21025213e-01,
	          3.99862331e-01,   5.04414970e-01,   5.90291842e-01,
	          6.44080948e-01,   6.90534178e-01,   6.95267597e-01,
	          6.83897746e-01,   6.87498635e-01,   7.08524933e-01,
	          7.39221369e-01,   7.41876381e-01,   7.41073635e-01,
	          7.30573061e-01,   7.49287966e-01,   7.72611111e-01,
	          7.56362235e-01,   7.29145904e-01,   7.32854084e-01,
	          7.45687712e-01,   7.52392093e-01,   7.85967737e-01,
	          7.81229753e-01,   7.78117630e-01,   7.83203167e-01,
	          7.82707772e-01,   7.59250688e-01,   7.40549288e-01,
	          7.03155545e-01,   6.74305018e-01,   6.83767910e-01,
	          7.23471478e-01,   7.32137681e-01,   7.23130451e-01,
	          7.06881795e-01,   7.19891807e-01,   7.28845055e-01,
	          7.19581001e-01,   7.14796031e-01,   7.14895592e-01,
	          7.14775295e-01,   7.12794350e-01,   6.93341780e-01,
	          6.82194342e-01,   6.68090094e-01,   6.63299059e-01,
	          6.52221849e-01,   6.54326664e-01,   6.77011343e-01,
	          6.87255981e-01,   7.18478957e-01,   7.32928278e-01,
	          7.63339253e-01,   7.59533327e-01,   9.88279016e-01,
	          2.26607605e-01,  -1.17261232e-03,   5.33979704e-01,
	         -1.43079290e-01,   6.98621909e-01,  -3.19442685e-01,
	          7.34556086e-02,  -5.18478307e-02,   1.24651059e-01,
	         -1.51868634e-01,   3.11164815e-01,  -1.44252297e-01,
	          4.16018279e+00,   1.01594252e+00,   4.59523232e+00,
	          8.37511412e-01,   4.85926071e+00,   1.01685969e+00,
	          5.73571811e+00,   1.10291769e+00,   5.20582051e+00,
	          6.52489411e-01,   5.87203241e+00,   1.06408092e+00,
	          6.10844893e+00,   1.75415293e+00]],
    "comment__": "Action scaling values to be used to scale values for the network",
"action_bounds":    [[0.200000, 1.800000, -0.497252, -1.993935, 0.073527,-0.321481, 0.800288, -3.876833, -1.180141, -0.776967, -1.810372, -2.142646, -3.406771, 0.240827, -2.373369, -2.308333, -0.563421, -2.891676, -1.918455, -1.842994, -2.262842, 0.053321, -0.666870, -1.994344, 0.014701, -1.908623, -1.955703, 0.034434],
									[0.351474, 2.300000, 0.097252, -0.693935, 1.873527, 3.121481, 2.500288, -2.576833, 0.580141, 0.176967, 0.610372, 0.642646, -0.006771, 2.240827, -0.573369, -0.008333, 0.863421, -1.091676, 0.418455, -0.242994, 0.262842, 0.853321, -0.166870, -0.494344, 1.394701, 0.908623, 1.655703, 2.634434]],
    "comment__": "Set of discrete actions that can be sampled from",				     
"discrete_actions": [[0.251474, 2.099796, -0.097252, -0.993935, 0.273527, 0.221481, 1.100288, -3.076833, 0.180141, -0.176967, 0.310372, -1.642646,   -0.406771, 1.240827, -1.773369, -0.508333, -0.063421, -2.091676, -1.418455, -1.242994, -0.262842, 0.453321, -0.366870, -1.494344, 0.794701, -1.408623, 0.655703, 0.634434],
		             [0.316504, 1.963316, -0.325309, -1.802222, 1.668542, 2.453011, 2.139391, -3.636978, -0.855670, -0.350402, -1.342939, 0.337384, -3.272340, 2.048047, -0.938193, -1.799840,   0.384958, -2.357088,    0.076791, -1.513792, -1.855033, 0.340609, -0.400898, -0.903512, 0.144128, 0.524855, -1.534278, 1.856180 ],
		             [0.289427, 1.994835, -0.254906, -1.167773, 0.275834, -0.197735, 2.298369, -3.272985, -0.179217, -0.530879, -0.362771, -0.690035, -0.558768, 1.806804, -1.341557, -0.688935, 0.337809, -1.773089,   -1.025832, -0.472297, -0.766728, 0.358616, -0.481754, -1.328176, 0.498767, -0.236538, -0.471444, 1.154247]],
    "comment__": "Is action space continuous or discrete?",
"action_space_continuous":true,
    "comment__": "Should the method train on the validation set only",
"train_on_validation_set":true,
    "comment__": "Name of the type of simulator to use",
"environment_type": "terrainRLBiped2D",
    "comment__": "Model type to use for the forward dynamics model",
"forward_dynamics_predictor": "network",
    "comment__": "Method to be used for the forward dynamics model is the model types uses a simulator",
"sampling_method": "SequentialMC",
    "comment__": "Use the action suggested by the policy to start the sampling method.",
"use_actor_policy_action_suggestion": true,
    "comment__": "If selecting from a uniform distribution the number of regularly distant samples to take / action dimension",
"num_uniform_action_samples": 3,
    "comment__": "Number of steps ahead the actions should be sampled",
"look_ahead_planning_steps": 2,
    "comment__": "How often to update the training data and plots wrt # of rounds",
"plotting_update_freq_num_rounds": 10,
    "comment__": "How often to save the training data and plotting data",
"saving_update_freq_num_rounds": 10,
    "comment__": "Number of treads that can be run in parallel during training",
"num_available_threads": 1,
    "comment__": "Length of the queues used to pass simulation data between the simulation workers and the learning agent(s).",
"queue_size_limit": 800,
    "comment__": "Number of actions performed between training updates",
"sim_action_per_training_update": 8,
    "comment__": "Number of rounds of adaptive sampling",
"adaptive_samples": 5,
    "comment__": "Number of elite adaptive samples to keep between adaptive sampling rounds",
"num_adaptive_samples_to_keep": 50,
    "comment__": "Use the variance calculated from the policy network (calculated using dropout)",
"use_actor_policy_action_variance_suggestion": false,
    "comment__": "Method used for action exploration",
"exploration_method": "gaussian_random",
    "comment__": "Amount of dropout to use in the networks (if using a dropout network)",
"dropout_p": 0.1,
    "comment__": "Regularization weight for the policy network",
"regularization_weight": 0.00001,
    "comment__": "Some parameter for rmsprop stocastic gradient optimization method.",
"rho": 0.95,
    "comment__": "Some parameter for rmsprop stocastic gradient optimization method.",
"rms_epsilon": 0.000001,
    "comment__": "Number of training updates before the target network is updated",
"steps_until_target_network_update": 500,
    "comment__": "Initial factor epsilon in multiplied by (This value will slowly be reduced during training)",
"epsilon_annealing": 0.8,
    "comment__": "Different ways of calculating the scaling method used normalize the input and outputs of the network from the bootstrapping samples. minmax, input and output are -mean/max-min. variance, input and output are -mean/(std*2), given, use the bounds provided in this file",
"state_normalization": "variance",
    "comment__": "load a pretrained model for the controller",
"load_saved_model": false,
	"comment__": "Number of updates the critic should perform per actor update",
"critic_updates_per_actor_update": 2,
    "comment__": "weather or not to clamp actions to stay inside the action boundaries",
"clamp_actions_to_stay_inside_bounds": true,
    "comment__": "Number of initial actions to sample before calculating input/output scaling and starting to train.",
"bootstrap_samples": 10000,
    "comment__": "What method to use to select actions during bootstrapping",
"bootsrap_with_discrete_policy": true,
    "comment__": "That max number of action that can be take before the end of an episode/epoch",
"max_epoch_length": 100,
    "comment__": "If reward is below this bound it will not be put in the Experience Buffer",
"reward_lower_bound": -0.5,
    "comment__": "Enable guided policy search. Uses MCMC sampling ahead in time to select the best action to keep",
"use_guided_policy_search" : false,
    "comment__": "The number of training updates to perform for every action that is simulated",
"training_updates_per_sim_action": 1,
    "comment__": "Use The forward dynamics simulator as a way of sampling suggested actions for exploration",
"use_sampling_exploration": false,
    "comment__": "Use the forward dyanmics model to perform action exploration wrt to V -> fd > delta Action gradients",
"use_model_based_action_optimization": true,
    "comment__": "Flag for policy evaluation to swap in the task network from one model and the character/robot network from another",
"use_transfer_task_network": false,
    "comment__": "Add a large cost to actions that are suggested to be outside the action bounds.",
"penalize_actions_outside_bounds": false,
    "comment__": "Network type to use for the forward dynamics model",
"forward_dynamics_model_type": "model.ForwardDynamicsDenseNetworkDropout.ForwardDynamicsDenseNetworkDropout",
    "comment__": "Whether or not to save the Experience memory after bootstrapping",
"save_experience_memory": false,
    "comment__": "Whether or not to train the policy and critic?",
"train_rl_learning": true,
    "comment__": "Force the character to start each new action in a good state, close to a good solution",
"use_back_on_track_forcing": false,
    "comment__": "draw/render the next state suggested by the forward dynamics model",
"visualize_forward_dynamics": false,
    "comment__": "Learning rate for the forward dynamics model",
"fd_learning_rate": 0.01,
    "comment__": "Whether or not to train the policy. Used for debugging",
"train_actor": true,
    "comment__": "Plot the terms for the critic as well (regularization and td error)",
"debug_critic": true,
    "comment__": "critic regularization weight",
"critic_regularization_weight": 0.00001,
    "comment__": "Critic learning rate",
"critic_learning_rate": 0.001,
    "comment__": "During evaluation plot of value function",
"visualize_expected_value": true,
    "comment__": "exponential decay value for use in reward function",
"target_velocity_decay":-1.25,
    "comment__": "Target velocity for controller",
"target_velocity":3.0,
    "comment__": "NUmmber of terrain features for which convolutinoal filters should be used",
"num_terrain_features": 200,
    "comment__": "Initial tempurature for annealing of e-greedy exploration",
"initial_temperature": 2.5,
    "comment__": "epsilon lower limit",
"min_epsilon": 0.15,
    "comment__": "Whether or not to draw/render the simulation",
"shouldRender": true,
    "comment__": "Learning rate use for model based action exploration",
"action_learning_rate": 1.0,
    "comment__": "During model-based action exloration, Probability of a random action being generated from MBAE,",
"model_based_action_omega": 0.2,
	"comment__": "visualize the loss and regularization cost of the actor. Warning: might use a lot of memory",
"debug_actor": true,
	"comment__": "float type to use, if for example you want to train on the GPU use float32",
"float_type": "float64",
	"comment__": "What processor type to perform the training on [cpu|gpu]",
"training_processor_type": "cpu",
	"comment__": "optimizer is the type of optimization algorithm to use",
"optimizer": "adam",
	"comment__": "This setting uses a sampling method overtop of simulation",
"use_simulation_sampling": false,
	"comment__": "Variance scaling used for sampling",
"variance_scalling": 0.1,
	"comment__": "Whether or not to parameterize the control via the reward function, also adds parameters to input state",
"use_parameterized_control": false,
	"comment__": "parameter bounds for parameterized controller",
"controller_parameter_settings": {
		"velocity_bounds": [[0.5],[2.5]]
		},
	"comment__": "The parameter used to control the average change in the control parameters",
"average_parameter_change": 0.25,
	"comment__": "Whether or not to train the value function some output from the forward dynamics",
"train_critic_on_fd_output": false,
	"comment__": "Use to add an additional regularization term to prevent the network from moving to far from its previous values",
"use_previous_value_regularization": false,
	"comment__": "Controls the level of information that is printed to the terminal",
"print_level": "train",
	"comment__": "print level descriptions",
"print_levels": {
		"debug": 1,
		"train": 0,
		"hyper_train": -1
		},
	"comment__": "The type of function to apply over the controller target values [gaussian|abs]",
"reward_smoother": "gaussian",
	"comment__": "Weights for different components of the reward function",
"controller_reward_weights": {
		"velocity": 0.8,
		"torque": 0.05,
		"root_height": 0.05,
		"root_pitch": 0.1,
		"right_hand_x_pos": 0.0
		},
	"comment__": "Regularization weight for different between policy parameters and old policy parameters",
"previous_value_regularization_weight":  0.001,
	"comment__": "Random seed value for the simulation to use",
"random_seed": 1234,
	"comment__": "Makes a few changes to the flow of control in order for things to be on policy",
"on_policy": false,
	"comment__": "Whether or not to use a stochastic policy, This adds more outputs to the network and changes the way actions are sampled",
"use_stocastic_policy": false,
	"comment__": "Whether or  not to train the critic at all. Usually used for debugging",
"train_critic": true,
	"comment__": "What type of regularization to use",
"regularization_type": "kl",
	"comment__": "Whether or not to collects tuples in batches, this can be good for multi-threading or computing furture discounted reward",
"collect_tuples_in_batches":false,
	"comment__": "Whether or not the controller should be reset to a new epoch when a fall (fallen into some kind of non-recoverable state) has occured",
"reset_on_fall": true,
	"comment__": "Whether a model of the reward r <- R(s,a) should be trained",
"train_reward_predictor": true,
	"comment__": "How many gradient steps model based action exploration should take",
"num_mbae_steps": 1,
    "comment__": "Whether or not the actor buffer should be fixed to process batches of a certain size",
"fix_actor_batch_size": false,
	"comment__": "Whether or not to put MBAE on the annelling schedule",
"anneal_mbae": false,
	"comment__": "The number of critic updates that are done between every dyna update step.",
"dyna_update_lag_steps": 1,
	"comment__": "Whether or not to use random actions for the MBAE exploration",
"use_random_actions_for_MBAE": false,
	"comment__": "Only use exploratory actions to update the policy",
"only_use_exp_actions_for_poli_updates": true,
	"comment__": "Multiply the MBAE action by a sample from a uniform distribution, to allow the action to vary in magnitude",
"randomize_MBAE_action_length": true,
	"comment__": "Only use exploratory actions to update the policy",
"only_use_exp_actions_for_poli_updates": true,
	"comment__": "Use generalized advantage estimation",
"use_GAE": true,
	"comment__": "generalized advantage estimation lambda in [0,1]",
"GAE_lambda": 0.95,
	"comment__": "Don't weight policy updates wrt the advantage of the action",
"dont_use_advantage": true,
	"comment__": "Whether or not to use a stochastic forward dynamics model",
"use_stochastic_forward_dynamics": true,
	"comment__": "Load a saved and pre-trained forward dynamics model",
"load_saved_fd_model": false,
	"comment__": "Whether ot not MBAE should optimize the q function or the advantage function",
"optimize_advantage_for_MBAE": false
}