{
"model_type": "Deep_NN",
"agent_name": "CACLA",
"data_folder": "Simple_Walk_Flat/",
"comment": "initial probability of selecting a random action",
"epsilon": 0.75, 
"comment": "initial probability of selecting a discrete random action",
"omega": 0.0,
"batch_size": 32,
"learning_rate": 0.001,
"sim_config_file": "./args/genBiped2D/opt_args_imitate_biped.txt",
"forwardDynamics_config_file": "./args/genBiped2D/opt_args_imitate_biped.txt",
"exploration_rate": 0.1,
"rounds": 5000,
"epochs": 10,
"eval_epochs": 10,
"discount_factor": 0.95,
"visualize_learning": true,
"save_trainData": true,
"train_forward_dynamics": false,
"visulaize_forward_dynamics": false,
"reward_bounds": [[0.0],[1.0]],
"expereince_length": 50000,
"state_bounds": [[ 0.11447661, -0.15172686,  0.05928519, -0.18942977, -0.24016868,
        -0.45861425, -0.58590659, -0.69572815, -0.81704971, -0.16059793,
        -0.20786422, -0.41100695, -0.54605169, -0.63260073, -0.82403291,
         0.2981021 , -2.61273618, -0.36947999, -2.01136596, -1.12600253,
        -1.38327546, -2.48557147, -1.10683957, -0.65551674, -1.92819284,
        -1.30037501, -1.32183658, -1.79792838, -1.0226148 ,  0.73256106,
         -0.1        ,  -0.19      , -0.08968808, -0.19315413, -0.32992601,
        -0.56065869, -0.5126534 , -0.80801399, -0.08417724, -0.19350833,
        -0.3239507 , -0.5548018 , -0.51876284, -0.80033999,  0.55423746,
        -0.48852306,  0.41000694, -0.652312  , -0.76558854, -1.05349903,
        -1.80297783, -1.33703541,  0.45870361, -0.69127251, -0.66011555,
        -1.16239401, -1.69101752, -1.47316526],
       [ 0.81286063,  0.20389506,  0.25734519,  0.14020653, -0.04786767,
         0.10952927,  0.10168192,  0.0210893 ,  0.37481127,  0.19852173,
        -0.08226689,  0.21751606,  0.00916153,  0.08253997,  0.28202565,
         2.48334281,  0.81095341,  2.34071815,  0.92931357,  2.7967646 ,
         1.3312622 ,  4.11294423,  1.96295564,  2.77277332,  0.75293098,
         3.20187861,  1.10067093,  3.5900505 ,  1.7446638 ,  0.79270804,
         0.1        ,  0.19      ,  0.15347745, -0.12593843,  0.33063181,
        -0.39026505,  0.44965946, -0.55521673,  0.16106179, -0.12205098,
         0.34450458, -0.38269787,  0.46548732, -0.54426242,  2.52632414,
         0.48779986,  2.62666038,  0.60865953,  3.67292998,  0.98132834,
         4.60341066,  1.2579612 ,  2.67571832,  0.67148   ,  3.88821045,
         1.0624396 ,  4.98167761,  1.3025929 ]],
				 
"action_bounds":    [[-0.37, -3.0, -0.57, -0.37, -3.0, -0.57],
				     [ 2.87,  0.0,   1.0,  2.87,  0.0,  1.0]],
				     
"discrete_actions": [[0.251474, 2.099796, -0.097252, -0.993935, 0.273527, 0.221481],
		             [0.1, -0.1, 0.12, -0.11, 0.02, 0.04],
		             [0.289427, 1.994835, -0.254906, -1.167773, 0.275834, -0.197735]],
"action_space_continuous":true,
"train_on_validation_set":true,
"environment_type": "terrainRLImitateBiped2D",
"forward_dynamics_predictor": "network",
"sampling_method": "SequentialMC",
"use_actor_policy_action_suggestion": true,
"num_uniform_action_samples": 3,
"look_ahead_planning_steps": 2,
"plotting_update_freq_num_rounds": 2,
"saving_update_freq_num_rounds": 2,
"num_available_threads": 5,
"queue_size_limit": 100,
"sim_action_per_training_update": 8,
"adaptive_samples": 5,
"num_adaptive_samples_to_keep": 50,
"use_actor_policy_action_variance_suggestion": false,
"exploration_method": "gaussian_random",
"dropout_p": 0.1,
"regularization_weight": 0.0001,
"rho": 0.95,
"rms_epsilon": 0.000001,
"steps_until_target_network_update": 1000,
"epsilon_annealing": 0.8,
"comment__": "Different ways of calculating the scaling method used normalize the input and outputs of the network from the bootstrapping samples. minmax, input and output are -mean/max-min. variance, input and output are -mean/(std*2), given, use the bounds provided in this file",
"state_normalization": "given",
    "comment__": "load a pretrained model for the controller",
"load_saved_model": false,
	"comment__": "Number of updates the critic should perform per actor update",
"critic_updates_per_actor_update": 5,
"clamp_actions_to_stay_inside_bounds": false,
"bootstrap_samples": 5000,
"bootsrap_with_discrete_policy": true,
"max_epoch_length": 200,
"reward_lower_bound": -0.5,
"use_guided_policy_search" : false,
"training_updates_per_sim_action": 1,
"use_sampling_exploration": false,
"use_model_based_action_optimization": true,
"use_transfer_task_network": false,
"penalize_actions_outside_bounds": false,
"use_transfer_task_network": false,
"forward_dynamics_model_type": "model.ForwardDynamicsDenseNetworkDropout.ForwardDynamicsDenseNetworkDropout",
"save_experience_memory": false,
"train_rl_learning": true,
"use_back_on_track_forcing": false,
"visualize_forward_dynamics": false,
"fd_learning_rate": 0.01,
"train_actor": true,
"debug_critic": false,
"critic_regularization_weight": 0.0001,
"critic_learning_rate": 0.01,
"visualize_expected_value": true,
"target_velocity_decay":-0.75,
"target_velocity":3.0,
"num_terrain_features": 0,
"initial_temperature": 20.0,
"min_epsilon": 0.15,
"shouldRender": false,
"action_learning_rate": 10.0,
"model_based_action_omega": 0.0
}